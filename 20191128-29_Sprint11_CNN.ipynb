{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint11 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1次元畳み込み層とは\n",
    ">CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from numpy import linalg as LA\n",
    "import copy\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# MNIST data set\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1, 784) #平滑化\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    ">チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、\n",
    "ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。\n",
    "1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、\n",
    "立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
    "\"\"\"\n",
    "\n",
    "#1次元の 畳み込み層 を作成\n",
    "#基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。\n",
    "#重みの初期化に関するクラスは必要に応じて作り変えてください。\n",
    "#Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "#ここでは パディング は考えず、ストライド も1に固定します。\n",
    "#また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。\n",
    "\n",
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, P, F_size, S, \n",
    "                 initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, F_size)\n",
    "        self.B = initializer.B()\n",
    "        self.F_size = F_size\n",
    "        self.X = None\n",
    "        self.X_array = None\n",
    "        self.N_out = int(N_out(n_nodes1, P, self.F_size, S))\n",
    "        #P パディング数\n",
    "        #S ストライド数\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        self.H_W = 0\n",
    "        self.H_B = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        一個ずつずらしたものを下にstackしていく。インデックスを取り出したものをぶち込んでいく。\n",
    "        →フィルターでdot積。それが求めたいaの行列。\n",
    "        フィルターの作り方は？\n",
    "        →フィルター = 重みパッケージ。つまり重みの集合体。\n",
    "        \"\"\"\n",
    "        #ストライドで一個ずつずらしたものをvstackした行列の作成\n",
    "        X_array_raw = np.zeros(len(self.W))\n",
    "        for i in range(self.N_out):\n",
    "            #Xのi番目からF_size分取り出してstack\n",
    "            filter_cut = X[i:i+self.F_size]\n",
    "            X_array_raw = np.vstack((X_array_raw,filter_cut))\n",
    "            \n",
    "        self.X_array = np.delete(X_array_raw, 0, 0) #0列目のzeros削除\n",
    "        #dot積\n",
    "        A = self.X_array @ self.W.reshape(len(self.W),1) + self.B #W(F_size),B(n_nodes2,)\n",
    "        \n",
    "        return A #出力 (len(self.X) - self.F_size + 1,1)\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (len(self.X) - self.F_size + 1, 1)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        N_out # 問題2 で作成 (len(self.X) - self.F_size + 1)\n",
    "        \n",
    "        dX : 次の形のndarray, shape (X.shape)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis=0)\n",
    "        self.dW = self.X_array.T @ dA\n",
    "        \"\"\"\n",
    "        端の処理\n",
    "        s は\n",
    "        a_s output のs番目(F_size 通り、０-s番目まである)\n",
    "        x_j Xのj番目\n",
    "        j - s < 0 →左端の時、filter 右側要素と被らないため\n",
    "        j - s > N_out - 1 →右端の時、filter 左側要素と被らないため\n",
    "        \n",
    "        →dA分出てくるので、それをsum\n",
    "        \"\"\"\n",
    "        dX = np.zeros_like(self.X)\n",
    "        for j in range(len(self.X)):\n",
    "            dXjs_list = []\n",
    "            for s in range(self.F_size):\n",
    "                if j - s < 0 or j - s > self.N_out - 1 : \n",
    "                    dXjs = 0\n",
    "                else:\n",
    "                    dXjs = dA[j-s] * self.W[s]\n",
    "                dXjs_list.append(dXjs)\n",
    "                \n",
    "            dXj = np.sum(dXjs_list) \n",
    "            dX[j] = dXj\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self) \n",
    "        return dX #X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期値セット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sprint10 初期値セット\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_type = 'sig', sigma=0.01):\n",
    "        self.weight_type = weight_type\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, F_size):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        F_size : int\n",
    "          フィルターサイズ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        if self.weight_type == 'sig':\n",
    "            W_std = self.sigma\n",
    "        \n",
    "        elif self.weight_type == 'xav':\n",
    "            W_std = 1/np.sqrt(n_nodes1)\n",
    "            \n",
    "        elif self.weight_type == 'he':\n",
    "            W_std = np.sqrt(2/n_nodes1)\n",
    "            \n",
    "        W = W_std * np.random.randn(F_size)\n",
    "        return W\n",
    "    \n",
    "    def B(self):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 共通。スカラー。\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.rand()\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "\n",
    "    def forward(self,A):\n",
    "        if A.ndim == 2:\n",
    "            x = A.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T\n",
    "        \n",
    "#         x = A - np.max(A) \n",
    "        # オーバーフロー対策\n",
    "#         Z_last = np.exp(A) / np.sum(np.exp(A))\n",
    "        return np.exp(A) / np.sum(np.exp(A))\n",
    "        \n",
    "#         return Z_last\n",
    "    \n",
    "    def backward(self,Z_last,y):\n",
    "        dA_last = Z_last - y #交差エントロピー誤差の計算も含む実装\n",
    "        return dA_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "合計が１になるか確認することが大切。\n",
    "\"\"\"\n",
    "soft_func = Softmax()\n",
    "soft = np.sum(soft_func.forward(X_train),axis=1)\n",
    "soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.A = None    \n",
    "\n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        Z = 1/ (1+ np.exp(- self.A))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        dA =dZ * ((1- (1/ (1+ np.exp(- self.A))))*(1/ (1+ np.exp(- self.A))))\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hipo:\n",
    "    def __init__(self):\n",
    "        self.A = None    \n",
    "\n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        dA = dZ * ((np.tanh(self.A))**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数 ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.forward_A = None\n",
    "    def forward(self, A):\n",
    "        self.forward_A = A\n",
    "        return np.maximum(A, 0)\n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.where(self.forward_A>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer): \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        '''\n",
    "        layer (FC)から持ってこれるようになる。\n",
    "        layer.Z = FC内のself.Z\n",
    "        layer.W\n",
    "        layer.B\n",
    "        '''\n",
    "        layer.W = layer.W - self.lr * layer.dW\n",
    "        layer.B = layer.B - self.lr * layer.dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # ミニバッチ方向(axis=0)にベクトルの平均を計算 #(n_nodes1, n_nodes2)\n",
    "        layer.H_W = layer.H_W + np.sum(layer.dW ** 2)\n",
    "        layer.H_B = layer.H_B + np.sum(layer.dB ** 2)\n",
    "        layer.W = layer.W - (self.lr * np.sqrt(1/layer.H_W)) * layer.dW\n",
    "        layer.B = layer.B - (self.lr * np.sqrt(1/layer.H_B)) * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    ">畳み込みを行うと特徴量の数が変化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "N_out : 出力のサイズ（特徴量の数）\n",
    "N_in : 入力のサイズ（特徴量の数）\n",
    "P : ある方向へのパディングの数\n",
    "F_size : フィルタのサイズ\n",
    "S : ストライドのサイズ\n",
    "\"\"\"\n",
    "\n",
    "def N_out(N_in, P, F_size, S):\n",
    "    out = (N_in + 2 * P - F_size)/ S + 1 #出力のサイズ（特徴量の数）\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    ">次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mondai3:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, P, F_size, S, \n",
    "                 initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = np.array([3, 5, 7])\n",
    "        self.B = np.array([1])\n",
    "\n",
    "        self.F_size = F_size\n",
    "        self.X = None\n",
    "        self.X_array = None\n",
    "        self.N_out = int(N_out(n_nodes1, P, self.F_size, S))\n",
    "        #P パディング数\n",
    "        #S ストライド数\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        self.H_W = 0\n",
    "        self.H_B = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        一個ずつずらしたものを下にstackしていく。インデックスを取り出したものをぶち込んでいく。\n",
    "        →フィルターでdot積。それが求めたいaの行列。\n",
    "        フィルターの作り方は？\n",
    "        →フィルター = 重みパッケージ。つまり重みの集合体。\n",
    "        \"\"\"\n",
    "        #ストライドで一個ずつずらしたものをvstackした行列の作成\n",
    "        X_array_raw = np.zeros(len(self.W))\n",
    "        for i in range(self.N_out):\n",
    "            #Xのi番目からF_size分取り出してstack\n",
    "            filter_cut = X[i:i+self.F_size]\n",
    "            X_array_raw = np.vstack((X_array_raw,filter_cut))\n",
    "            \n",
    "        self.X_array = np.delete(X_array_raw, 0, 0) #0列目のzeros削除\n",
    "        print(\"self.X_array\",self.X_array)\n",
    "        #dot積\n",
    "        A = self.X_array @ self.W.reshape(len(self.W),1) + self.B #W(F_size),B(n_nodes2,)\n",
    "        \n",
    "        return A #出力 (len(self.X) - self.F_size + 1,1)\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (len(self.X) - self.F_size + 1, 1)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        N_out # 問題2 で作成 (len(self.X) - self.F_size + 1)\n",
    "        \n",
    "        dX : 次の形のndarray, shape (X.shape)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.array([30])\n",
    "        self.dW = np.array([50, 80, 110])\n",
    "        \"\"\"\n",
    "        端の処理\n",
    "        s は\n",
    "        a_s output のs番目(F_size 通り、０-s番目まである)\n",
    "        x_j Xのj番目\n",
    "        j - s < 0 →左端の時、filter 右側要素と被らないため\n",
    "        j - s > N_out - 1 →右端の時、filter 左側要素と被らないため\n",
    "        \n",
    "        →dA分出てくるので、それをsum\n",
    "        \"\"\"\n",
    "        dX = np.zeros_like(self.X)\n",
    "        for j in range(len(self.X)):\n",
    "            dXjs_list = []\n",
    "            for s in range(self.F_size):\n",
    "                if j - s < 0 or j - s > self.N_out - 1 : \n",
    "                    dXjs = 0\n",
    "                else:\n",
    "                    dXjs = dA[j-s] * self.W[s]\n",
    "                dXjs_list.append(dXjs)\n",
    "                \n",
    "            dXj = np.sum(dXjs_list) \n",
    "            dX[j] = dXj\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self) \n",
    "        return dX #X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力x、重みw、バイアスbを次のようにします。\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "\n",
    "\n",
    "\n",
    "#フォワードプロパゲーションをすると出力は次のようになります。\n",
    "a = np.array([35, 50])\n",
    "\n",
    "#次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "delta_a = np.array([10, 20])\n",
    "\n",
    "#バックプロパゲーションをすると次のような値になります。\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple3 = mondai3(n_nodes1=4, n_nodes2=3, P=0, F_size = 3, S=1,\n",
    "                 initializer = SimpleInitializer(weight_type = 'sig', sigma=0.01),\n",
    "                 optimizer = SGD(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.X_array [[1. 2. 3.]\n",
      " [2. 3. 4.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[35.],\n",
       "       [50.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple3.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30, 110, 170, 140])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple3.backward(delta_a) #array([ 30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx1 [0 1 2]\n",
      "idx2 [[0]\n",
      " [1]]\n",
      "index [[0 1 2]\n",
      " [1 2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([35, 50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for 文を使わずに行いたい時\n",
    "\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "idx1 = np.arange(w.shape[0])\n",
    "idx2 = np.arange(w.shape[0] - 1).reshape(-1, 1)\n",
    "index = idx1 + idx2\n",
    "print('idx1',idx1)\n",
    "print('idx2',idx2)\n",
    "print('index',index)\n",
    "a = np.dot(x[index], w.T) + b\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装上の工夫\n",
    ">xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "\n",
    "a = np.empty((2, 3))\n",
    "\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "\n",
    "a = a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nこのこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\\n畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "#また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
    "\"\"\"\n",
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    ">チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-87afe45294ee>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-87afe45294ee>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    w_filters = np.delete(w_filters,0,0).reshape(w_filters[])\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "w = np.ones((3, 2, 3))\n",
    "print(\"w\",w.shape) # shape(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "#Filter 加工,チャネル1の下に\n",
    "\n",
    "w_filters = np.zeros_like(w[0,:,:].flatten())\n",
    "for i in range(w.shape[0]):\n",
    "    w_filters = np.vstack((w_filters, w[i,:,:].flatten()))\n",
    "\n",
    "w_filters = np.delete(w_filters,0,0).reshape(w_filters[])\n",
    "print(\"w_filters\",w_filters)\n",
    "\n",
    "b = np.array([1, 2, 3]) \n",
    "b = b.reshape(len(b),1) # reshape（出力チャンネル数,1）\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [2 3 4 5]]\n",
      "X_array (2, 6)\n",
      "[[1. 2. 3. 2. 3. 4.]\n",
      " [2. 3. 4. 3. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
    "print(x) # shape(2, 4)で、（入力チャンネル数、特徴量数）\n",
    "\n",
    "X_array_raw = np.zeros((x.shape[0],1))\n",
    "for i in range(2):\n",
    "    #Xのi番目からF_size分取り出してstack\n",
    "    filter_cut = x[:,i:i+3]\n",
    "    X_array_raw = np.hstack((X_array_raw,filter_cut))\n",
    "X_array = np.delete(X_array_raw, 0, 1) #0列目のzeros削除\n",
    "print(\"X_array\",X_array.shape)\n",
    "print(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_pre:\n",
    "    \"\"\"\n",
    "    チャンネル数を1に限定しない1次元畳み込み層のクラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, channel_1, channel_2, P, F_size, S, \n",
    "                 initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = np.array([[[1,1,2],[2,1,1]],[[2,1,1],[1,1,1]],[[1,1,1],[1,1,1]]]) \n",
    "        self.B = np.array([1, 2, 3])\n",
    "\n",
    "        self.F_size = F_size\n",
    "        self.S = S\n",
    "        self.X = None\n",
    "        self.X_array_back = np.zeros_like((self.W[0,:,:])) #backward用\n",
    "        self.X_array = None\n",
    "        self.N_out = int(N_out(channel_1, P, self.F_size, S))\n",
    "        #P パディング数\n",
    "        #S ストライド数\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        self.H_W = 0\n",
    "        self.H_B = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        一個ずつずらしたものを下にstackしていく。インデックスを取り出したものをぶち込んでいく。\n",
    "        →フィルターでdot積。それが求めたいaの行列。\n",
    "        フィルターの作り方は？\n",
    "        →フィルター = 重みパッケージ。つまり重みの集合体。\n",
    "        \"\"\"\n",
    "        w_filters = np.zeros_like(self.W[0,:,:].flatten())\n",
    "        for i in range(self.W.shape[0]):\n",
    "            w_filters = np.vstack((w_filters, self.W[i,:,:].flatten()))\n",
    "        w_filters = np.delete(w_filters,0,0) #filter making\n",
    "\n",
    "        \n",
    "        #ストライドで一個ずつずらしたものをvstackした行列の作成\n",
    "        X_array_raw = np.zeros((self.X.shape[0],1)) #forward用\n",
    "        for i in range(self.N_out):\n",
    "            filter_cut = X[:,(i * self.S):(i * self.S + self.F_size)]\n",
    "            X_array_raw = np.hstack((X_array_raw,filter_cut))\n",
    "            self.X_array_back = np.vstack((self.X_array_back,filter_cut))\n",
    "        self.X_array = np.delete(X_array_raw, 0, 1) #0行目のzeros削除\n",
    "        self.X_array_back = np.delete(self.X_array_back, [0,1], 0)\n",
    "        \n",
    "        print(\"self.N_out\",self.N_out)\n",
    "        print(\"self.X_array.T\",self.X_array.T.shape)\n",
    "        print(\"w_filters\",w_filters.shape)\n",
    "        print(\"self.B\",self.B.shape)\n",
    "\n",
    "        #dot積\n",
    "        A = w_filters @ self.X_array.T + self.B.reshape(3,1) #W(F_size),B(n_nodes2,)\n",
    "        \n",
    "        return A #出力 (N_out,出力フィルタサイズ)\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (len(self.X) - self.F_size + 1, 1)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        N_out # 問題2 で作成 (len(self.X) - self.F_size + 1)\n",
    "        \n",
    "        dX : 次の形のndarray, shape (X.shape)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis=1)\n",
    "        \"\"\"\n",
    "        端の処理\n",
    "        s は\n",
    "        a_s output のs番目(F_size 通り、０-s番目まである)\n",
    "        x_j Xのj番目\n",
    "        j - s < 0 →左端の時、filter 右側要素と被らないため\n",
    "        j - s > N_out - 1 →右端の時、filter 左側要素と被らないため\n",
    "        \n",
    "        →dA分出てくるので、それをsum\n",
    "        \"\"\"\n",
    "        dW_raw = np.zeros_like(self.W[0])\n",
    "        X_array_back = np.split(self.X_array_back, 2)[0]\n",
    "        X_array_back_split = np.split(X_array_back, 2) #N_out 等分する\n",
    "        for i in range(dA.shape[0]): \n",
    "            dW_1 = dA[i,:] @ X_array_back_split[0] #dA[i,:]@X_array_back_split[j]\n",
    "            dW_2 = dA[i,:] @ X_array_back_split[1] #dA[i,:]@X_array_back_split[j]\n",
    "            dW_i = np.block([[dW_1], [dW_2]])\n",
    "            dW_raw = np.vstack((dW_raw, dW_i)) # stack していく\n",
    "\n",
    "        self.dW = np.delete(dW_raw, [0,1] , 0).reshape(Con1.W.shape) \n",
    "        print(\"dW\",self.dW) #(3,2,3)\n",
    "\n",
    "\n",
    "        dX_raw = np.zeros_like(self.X)\n",
    "        w_split = np.split(self.W, self.F_size) #F_size 等分する\n",
    "        # print(loss[0,:]) # [ 9 11]\n",
    "        for i in range(dA.shape[0]):\n",
    "            dX_1 = (dA[i,0] * w_split[i]).reshape(2,3) #(入力チャンネル数,F_size)\n",
    "            dX_2 = (dA[i,1] * w_split[i]).reshape(2,3)\n",
    "            dX_1 = np.append(dX_1, np.zeros((2,1)), axis=1)\n",
    "            dX_2 = np.append(np.zeros((2,1)), dX_2, axis=1)\n",
    "            dX_raw_i = dX_1+dX_2\n",
    "            #3次元方向にstackさせてく。３次元方向でsum\n",
    "            dX_raw = np.block([[dX_raw], [dX_raw_i]])\n",
    "\n",
    "        dX1 = np.sum(dX_raw[::2],axis=0)\n",
    "        dX2 = np.sum(dX_raw[1::2],axis=0)\n",
    "        dX = np.vstack((dX1,dX2))\n",
    "        print(\"dX\",dX)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self) \n",
    "        return dX #X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con1 = Conv1d_pre(channel_1=4, channel_2=3, P=0, F_size = 3, S=1,\n",
    "                  initializer = SimpleInitializer_CNN(weight_type = 'sig', sigma=0.01),\n",
    "                  optimizer = SGD(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con = Conv1d(channel_1=4, channel_2=3, P=0, F_size = 3, S=1,\n",
    "              initializer = SimpleInitializer_CNN(weight_type = 'sig', sigma=0.01),\n",
    "              optimizer = SGD(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.N_out 2\n",
      "self.X_array.T (6, 2)\n",
      "w_filters (3, 6)\n",
      "self.B (3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[21., 29.],\n",
       "       [18., 25.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#w、lossを白板と同じように作る。backward 動いてるか確認。\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
    "\n",
    "Con1.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Con1.X_array_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW [[[ 31  51  71]\n",
      "  [ 51  71  91]]\n",
      "\n",
      " [[102 169 236]\n",
      "  [169 236 303]]\n",
      "\n",
      " [[164 272 380]\n",
      "  [272 380 488]]]\n",
      "dX [[125. 230. 204. 113.]\n",
      " [102. 206. 195. 102.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[125., 230., 204., 113.],\n",
       "       [102., 206., 195., 102.]])"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = np.array([[9,11],[32,35],[52,56]])\n",
    "Con1.backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW [[[ 31.  51.  71.]\n",
      "  [ 51.  71.  91.]]\n",
      "\n",
      " [[102. 169. 236.]\n",
      "  [169. 236. 303.]]\n",
      "\n",
      " [[164. 272. 380.]\n",
      "  [272. 380. 488.]]]\n",
      "dX [[125. 230. 204. 113.]\n",
      " [102. 206. 195. 102.]]\n"
     ]
    }
   ],
   "source": [
    "loss = np.array([[9,11],[32,35],[52,56]])\n",
    "\n",
    "\n",
    "dW_raw = np.zeros_like(Con1.W[0])\n",
    "X_array_back = np.split(Con1.X_array_back, 2)[0]\n",
    "X_array_back_split = np.split(X_array_back, 2) #N_out 等分する\n",
    "#print(\"X_array_back_split\",X_array_back_split)\n",
    "#print(X_array_back_split[0])\n",
    "for i in range(loss.shape[0]): \n",
    "    dW_1 = loss[i,:] @ X_array_back_split[0] #loss[i,:]@X_array_back_split[j]\n",
    "    dW_2 = loss[i,:] @ X_array_back_split[1] #loss[i,:]@X_array_back_split[j]\n",
    "    dW_i = np.block([[dW_1], [dW_2]])\n",
    "    dW_raw = np.vstack((dW_raw, dW_i)) # stack していく\n",
    "#     dx[:,i] += col_dX_3d[:,:,i] #2d->3d\n",
    "\n",
    "dW = np.delete(dW_raw, [0,1] , 0).reshape(Con1.W.shape) \n",
    "print(\"dW\",dW) #(3,2,3)\n",
    "\n",
    "\n",
    "dX_raw = np.zeros_like(x)\n",
    "w_split = np.split(w, 3) #F_size 等分する\n",
    "# print(loss[0,:]) # [ 9 11]\n",
    "for i in range(loss.shape[0]):\n",
    "    dX_1 = (loss[i,0] * w_split[i]).reshape(2,3) #(入力チャンネル数,F_size)\n",
    "    dX_2 = (loss[i,1] * w_split[i]).reshape(2,3)\n",
    "    dX_1 = np.append(dX_1, np.zeros((2,1)), axis=1)\n",
    "    dX_2 = np.append(np.zeros((2,1)), dX_2, axis=1)\n",
    "    dX_raw_i = dX_1+dX_2\n",
    "    #3次元方向にstackさせてく。３次元方向でsum\n",
    "    dX_raw = np.block([[dX_raw], [dX_raw_i]])\n",
    "\n",
    "dX1 = np.sum(dX_raw[::2],axis=0)\n",
    "dX2 = np.sum(dX_raw[1::2],axis=0)\n",
    "dX = np.vstack((dX1,dX2))\n",
    "print(\"dX\",dX)\n",
    "\n",
    "# loss.flatten()\n",
    "# Con1.backward(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w、lossを白板と同じように作る。backward 動いてるか確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#例えば以下のようなx, w, bがあった場合は、\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
    "# shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.array([[[1,1,2],[2,1,1]],[[2,1,1],[1,1,1]],[[1,1,1],[1,1,1]]]) \n",
    "# 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "\n",
    "#出力は次のようになります。\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) \n",
    "# shape(3, 2)で、（出力チャンネル数、特徴量数）である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】（アドバンス課題）パディングの実装\n",
    ">畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】（アドバンス課題）ミニバッチへの対応\n",
    ">ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練用データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, \n",
    "                 seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    \n",
    "    \n",
    "# get_mini_batch = GetMiniBatch(X_train,y_train_one_hot,batch_size=20)\n",
    "# for mini_X_train, mini_y_train in get_mini_batch:\n",
    "# # このfor文内でミニバッチが使える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】（アドバンス課題）任意のストライド数\n",
    ">ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward の以下の部分\n",
    "#filter_cut = X[:,(i * self.S):(i * self.S + self.F_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定\n",
    ">これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.Z = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.Z = X\n",
    "        self.H_W = 0\n",
    "        self.H_B = 0\n",
    "        print(\"FC_X\",self.Z.shape)\n",
    "        print(\"W\",self.W.shape)\n",
    "        print(\"B\",self.B.shape)\n",
    "        \n",
    "        A = self.Z @ self.W + self.B #W(n_nodes1, n_nodes2),B(n_nodes2,)\n",
    "        \n",
    "        return A #出力 (batch_size, n_nodes2)\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis=0)\n",
    "        self.dW = self.Z.T @ dA\n",
    "        dZ = dA @ self.W.T\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self) \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer_fullyconnect:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_type = 'sig', sigma=0.01):\n",
    "        self.weight_type = weight_type\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        if self.weight_type == 'sig':\n",
    "            W_std = self.sigma\n",
    "        \n",
    "        elif self.weight_type == 'xav':\n",
    "            W_std = 1/np.sqrt(n_nodes1)\n",
    "            \n",
    "        elif self.weight_type == 'he':\n",
    "            W_std = np.sqrt(2/n_nodes1)\n",
    "            \n",
    "        W = W_std * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_out(N_in, P, F_size, S):\n",
    "    out = (N_in + 2 * P - F_size)/ S + 1 #出力のサイズ（特徴量の数）\n",
    "    return out\n",
    "\n",
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    チャンネル数を1に限定しない1次元畳み込み層のクラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    channel_1 : int\n",
    "      入力チャンネル数\n",
    "    channel_2 : int\n",
    "      出力チャンネル数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, channel_1, channel_2, P, F_size, S, \n",
    "                 initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(channel_2, channel_1, F_size) \n",
    "        #(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "        self.B = initializer.B(channel_2)\n",
    "        # reshape（出力チャンネル数,1）\n",
    "\n",
    "        self.F_size = F_size\n",
    "        self.S = S\n",
    "        self.X = None\n",
    "        self.X_array = None\n",
    "        self.X_array_back = np.zeros_like((self.W[0,:,:])) #backward用\n",
    "        self.N_out = int(N_out(channel_1, P, self.F_size, self.S))\n",
    "        self.channel_2 = channel_2\n",
    "        #P パディング数\n",
    "        #S ストライド数\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (channel_1 = batch_size(サンプル数), n_node1)\n",
    "            入力（入力チャンネル数、特徴量数）\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (channel_2, conv時間軸=N_out)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        self.H_W = 0\n",
    "        self.H_B = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        一個ずつずらしたものを下にstackしていく。インデックスを取り出したものをぶち込んでいく。\n",
    "        →フィルターでdot積。それが求めたいaの行列。\n",
    "        フィルターの作り方は？\n",
    "        →フィルター = 重みパッケージ。つまり重みの集合体。\n",
    "        \"\"\"\n",
    "        w_filters = np.zeros_like(self.W[0,:,:].flatten())\n",
    "        for i in range(self.W.shape[0]):\n",
    "            w_filters = np.vstack((w_filters, self.W[i,:,:].flatten()))\n",
    "        w_filters = np.delete(w_filters,0,0) \n",
    "        #filter flatに(channel_2, channel_1*F_size)\n",
    "\n",
    "        \n",
    "        #ストライドで一個ずつずらしたものをvstackした行列の作成\n",
    "        X_array_raw = np.zeros((self.X.shape[0],1)) #forward用\n",
    "        for i in range(self.N_out):\n",
    "            filter_cut = X[:,(i * self.S):(i * self.S + self.F_size)]\n",
    "            X_array_raw = np.hstack((X_array_raw,filter_cut))\n",
    "            self.X_array_back = np.vstack((self.X_array_back,filter_cut))\n",
    "        self.X_array = np.delete(X_array_raw, 0, 1) #0行目のzeros削除\n",
    "        self.X_array_back = np.delete(self.X_array_back, [0,1], 0)\n",
    "\n",
    "        #dot積\n",
    "        A = w_filters @ self.X_array.T + self.B \n",
    "        #A (channel_2, conv時間軸=N_out)\n",
    "        #self.X_array.T(F_size*channel_1,N_out), \n",
    "        #W(channel_2, channel_1*F_size), B(channel_2,1)\n",
    "        '''\n",
    "        backward reshape用にshapeh保持\n",
    "        '''\n",
    "        self.shape4reshape = A.shape\n",
    "        \n",
    "        A = A.reshape(1,self.channel_2 * self.N_out) #今回はここで平滑化\n",
    "        return A #出力(1,self.channel_2 * self.N_out)\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (len(self.X) - self.F_size + 1, 1)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        N_out # 問題2 で作成 (len(self.X) - self.F_size + 1)\n",
    "        \n",
    "        dX : 次の形のndarray, shape (X.shape)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis=1)\n",
    "        \n",
    "        dA = dA.reshape(self.shape4reshape)\n",
    "        \n",
    "        \"\"\"\n",
    "        端の処理\n",
    "        s は\n",
    "        a_s output のs番目(F_size 通り、０-s番目まである)\n",
    "        x_j Xのj番目\n",
    "        j - s < 0 →左端の時、filter 右側要素と被らないため\n",
    "        j - s > N_out - 1 →右端の時、filter 左側要素と被らないため\n",
    "        \n",
    "        →dA分出てくるので、それをsum\n",
    "        \"\"\"\n",
    "        dW_raw = np.zeros_like(self.W[0])\n",
    "        X_array_back = np.split(self.X_array_back, 2)[0]\n",
    "        X_array_back_split = np.split(X_array_back, 2) #N_out 等分する\n",
    "        for i in range(dA.shape[0]): \n",
    "            dW_1 = dA[i,:] @ X_array_back_split[0] #dA[i,:]@X_array_back_split[j]\n",
    "            dW_2 = dA[i,:] @ X_array_back_split[1] #dA[i,:]@X_array_back_split[j]\n",
    "            dW_i = np.block([[dW_1], [dW_2]])\n",
    "            dW_raw = np.vstack((dW_raw, dW_i)) # stack していく\n",
    "\n",
    "        self.dW = np.delete(dW_raw, [0,1] , 0).reshape(Con1.W.shape) \n",
    "        print(\"dW\",self.dW) #W.shape\n",
    "\n",
    "\n",
    "        dX_raw = np.zeros_like(self.X)\n",
    "        w_split = np.split(self.W, self.F_size) #F_size 等分する\n",
    "        # print(loss[0,:]) # [ 9 11]\n",
    "        for i in range(dA.shape[0]):\n",
    "            dX_1 = (dA[i,0] * w_split[i]).reshape(2,3) #(入力チャンネル数,F_size)\n",
    "            dX_2 = (dA[i,1] * w_split[i]).reshape(2,3)\n",
    "            dX_1 = np.append(dX_1, np.zeros((2,1)), axis=1)\n",
    "            dX_2 = np.append(np.zeros((2,1)), dX_2, axis=1)\n",
    "            dX_raw_i = dX_1+dX_2\n",
    "            #3次元方向にstackさせてく。３次元方向でsum\n",
    "            dX_raw = np.block([[dX_raw], [dX_raw_i]])\n",
    "\n",
    "        dX1 = np.sum(dX_raw[::2],axis=0)\n",
    "        dX2 = np.sum(dX_raw[1::2],axis=0)\n",
    "        dX = np.vstack((dX1,dX2))\n",
    "        print(\"dX\",dX)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self) \n",
    "        return dX #X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sprint10 初期値セット\n",
    "class SimpleInitializer_CNN:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_type = 'sig', sigma=0.01):\n",
    "        self.weight_type = weight_type\n",
    "        self.sigma = sigma\n",
    "    def W(self, channel_1, channel_2, F_size):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        #(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "        Parameters\n",
    "        ----------\n",
    "        channel_1 : int\n",
    "          入力チャンネル数\n",
    "        channel_2 : int\n",
    "          出力チャンネル数\n",
    "        F_size : int\n",
    "          フィルターサイズ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        if self.weight_type == 'sig':\n",
    "            W_std = self.sigma\n",
    "        \n",
    "        elif self.weight_type == 'xav':\n",
    "            W_std = 1/np.sqrt(channel_1)\n",
    "            \n",
    "        elif self.weight_type == 'he':\n",
    "            W_std = np.sqrt(2/channel_1)\n",
    "            \n",
    "        W = W_std * np.random.randn(channel_2, channel_1, F_size)\n",
    "        return W #(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "    \n",
    "    def B(self, channel_2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        channel_2 : int\n",
    "          出力チャンネル数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : # （出力チャンネル数,1）\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(channel_2)\n",
    "        B = B.reshape(len(B),1)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクラッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "N_out : 出力のサイズ（特徴量の数）\n",
    "N_in : 入力のサイズ（特徴量の数）\n",
    "P : ある方向へのパディングの数\n",
    "F_size : フィルタのサイズ\n",
    "S : ストライドのサイズ\n",
    "\"\"\"\n",
    "def N_out(N_in, P, F_size, S):\n",
    "    return (N_in + 2 * P - F_size)/ S + 1 #出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "class Scratch1dCNNClassifier():\n",
    "    \"\"\"\n",
    "    CNN 分類器\n",
    "\n",
    "    Parameters\n",
    "    ------    \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=5,\n",
    "                 lr=0.01, sigma = 0.01,\n",
    "                 verbose = True):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.iter : 学習回数\n",
    "        \"\"\"\n",
    "        self.iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def loss_func(self,y,y_pred_proba):\n",
    "        sigma_c = np.sum(y * np.log(y_pred_proba), axis = 1) \n",
    "        #batch_size方向に平均\n",
    "        return -np.mean(sigma_c)\n",
    "    \n",
    "    def fit(self, X, y,\n",
    "            sigma = 0.01,\n",
    "            n_output = 10):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_output = n_output\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1,\n",
    "                      SimpleInitializer(weight_type='xav',sigma = self.sigma),optimizer)\n",
    "        self.activation1 = Softmax()\n",
    "        self.loss = None\n",
    "\n",
    "        self.channel_1 = X.shape[0]\n",
    "        self.channel_2 = 1\n",
    "        \n",
    "        optimizer = SGD(self.lr) #更新方法選択\n",
    "        \n",
    "        self.Conv1 = Conv1d(channel_1= self.channel_1, \n",
    "                            channel_2= self.channel_2, \n",
    "                            P=0, F_size = 3, S=1,\n",
    "                            initializer\n",
    "                            = SimpleInitializer_CNN(weight_type = 'sig', \n",
    "                                                    sigma=0.01),\n",
    "                            optimizer = SGD(self.lr))\n",
    "        \n",
    "        \"\"\"\n",
    "        ノード　＝　チャンネル数(1にすると平滑化が楽)　✖️　チャンネルの要素数 x 次元数(今回は1)\n",
    "        出チャンネル数を　（平滑化）　\n",
    "        \"\"\"\n",
    "        for n in range(self.iter): #何回回そう？ MNISTなら50回ほどで試す\n",
    "            get_mini_batch = GetMiniBatch(X_train,\n",
    "                                          y_train_one_hot,\n",
    "                                          batch_size = 1)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "            # このfor文内でミニバッチが使える\n",
    "                #W,B = FC内 FCX.W, FCX.B (Xは1,2,...)\n",
    "                A1 = self.Conv1.forward(mini_X_train) \n",
    "                #(batch_size=1, n_nodes1 = N_out * 1d * channel_2)\n",
    "                Z1 = self.activation1.forward(A1) #(batch_size, n_nodes1)\n",
    "                A2 = self.FC1.forward(Z1) #(batch_size, n_nodes1)\n",
    "                Z2 = self.activation2.forward(A2) #(batch_size, n_output)                \n",
    "                \n",
    "                dA2 = self.activation2.backward(Z2, mini_y_train) #(batch_size, n_output)\n",
    "                # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dX1 = self.FC1.backward(dA2) #(batch_size, n_nodes1)\n",
    "                \n",
    "                #flattenで保持したshapeでreshape\n",
    "                \n",
    "                dA1 = self.activation1.backward(dX1) #(batch_size, n_nodes1)\n",
    "                dX0 = self.Conv1.backward(dA1) # dZ0は使用しない\n",
    "            \n",
    "            A1 = self.Conv1.forward(X) #(batch_size, n_nodes1)\n",
    "            Z1 = self.activation1.forward(A1) #(batch_size, n_nodes1)\n",
    "#             Z1 = np.flatten\n",
    "\n",
    "            A2 = self.FC1.forward(Z1) #(batch_size, n_nodes2)\n",
    "            Z2 = self.activation2.forward(A2) #(batch_size, n_nodes2)                \n",
    "            y_pred_proba = Z2\n",
    "            loss = self.loss_func(y,y_pred_proba)\n",
    "            self.loss = np.append(self.loss,loss)\n",
    "            \n",
    "        self.loss = np.delete(self.loss, 0)\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print(\"self.loss.shape\",self.loss.shape)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.FC1.forward(X) #(n_samples, n_nodes1)\n",
    "        Z1 = self.activation1.forward(A1) #(n_samples, n_nodes1)\n",
    "        A2 = self.FC2.forward(Z1) #(n_samples, n_nodes2)\n",
    "        Z2 = self.activation2.forward(A2) #(n_samples, n_nodes2)\n",
    "        A3 = self.FC3.forward(Z2) #(n_samples, n_output)\n",
    "        Z3 = self.activation3.forward(A3) #(n_samples, n_output)\n",
    "\n",
    "        y_pred_proba = np.argmax(Z3,axis=1)\n",
    "        return y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN1d = Scratch1dCNNClassifier(max_iter=1,\n",
    "                               lr=0.001, sigma = 0.01,\n",
    "                               verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 10)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot.shape #(48000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape #(サンプル数=48000, 784) \n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC_X (20, 3)\n",
      "W (3, 10)\n",
      "B (10,)\n",
      "[[[0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]]]\n",
      "dW_1 (5,)\n",
      "dW_2 (5,)\n",
      "dW_raw (3, 1, 3)\n",
      "dW_raw [[[0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]]]\n",
      "dW_i (2, 5)\n",
      "dW_i [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1025-c625b8da1436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCNN1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processing time:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1022-27192a33459e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sigma, n_output)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mdX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size, n_nodes2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mdA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdX1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size, n_nodes1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mdX0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dZ0は使用しない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size, n_nodes1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1019-2b2472185dff>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dA)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dW_i\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdW_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dW_i\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdW_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mdW_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stack していく\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCon1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "CNN1d.fit(X_train, y_train_one_hot)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"processing time:\",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN1d.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC_X (10000, 784)\n",
      "W (3, 10)\n",
      "B (10,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1010-c29ab41cddea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1006-27192a33459e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0m推定結果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(n_samples, n_nodes1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(n_samples, n_nodes1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(n_samples, n_nodes2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-834-5aa6bf9beef0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;31m#W(n_nodes1, n_nodes2),B(n_nodes2,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m \u001b[0;31m#出力 (batch_size, n_nodes2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 784)"
     ]
    }
   ],
   "source": [
    "y_pred = CNN1d.predict(X_test)\n",
    "\n",
    "np.set_printoptions(edgeitems= 10)\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(\"accuracy\",accuracy_score(np.argmax(y_test_one_hot,axis=1),y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x, y, and format string must not be None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1011-06700e179cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CNN1d.loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.plot(slr.val_loss,label='slr.val_loss', color = 'orange')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m                                 \u001b[0;31m# グリッド線を表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     return gca().plot(\n\u001b[1;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2795\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# downstream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x, y, and format string must not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x, y, and format string must not be None"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(CNN1d.loss,label='CNN1d.loss', color = 'blue')\n",
    "# plt.plot(slr.val_loss,label='slr.val_loss', color = 'orange')\n",
    "plt.grid(True)                                 # グリッド線を表示\n",
    "plt.legend()\n",
    "\"\"\"\n",
    "過学習対策にtrainloss曲線もplotしておく？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "以下Sprint１０\n",
    "\"\"\"\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    Deep Neural Netrowk 分類器\n",
    "\n",
    "    Parameters\n",
    "    ------    \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_iter=5,\n",
    "                 lr=0.01, sigma = 0.01,\n",
    "                 verbose = True):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.iter : 学習回数\n",
    "        \"\"\"\n",
    "        self.iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def loss_func(self,y,y_pred_proba):\n",
    "        sigma_c = np.sum(y * np.log(y_pred_proba), axis = 1) \n",
    "        #batch_size方向に平均\n",
    "        return -np.mean(sigma_c)\n",
    "    \n",
    "    def fit(self, X, y,\n",
    "            sigma = 0.01,n_nodes1 = 400,n_nodes2 = 200,\n",
    "            n_output = 10, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        optimizer = SGD(self.lr) #更新方法選択\n",
    "        \n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1,\n",
    "                      SimpleInitializer(weight_type='xav',sigma = self.sigma),optimizer)\n",
    "        self.activation1 = Sigmoid()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, \n",
    "                      SimpleInitializer(weight_type='xav',sigma = self.sigma),optimizer)\n",
    "        self.activation2 = Sigmoid()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, \n",
    "                      SimpleInitializer(weight_type='xav',sigma = self.sigma),optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "        self.loss = None\n",
    "        \n",
    "        for n in range(self.iter): #何回回そう？ MNISTなら50回ほどで試す\n",
    "            get_mini_batch = GetMiniBatch(X_train,\n",
    "                                          y_train_one_hot,\n",
    "                                          batch_size=20)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "            # このfor文内でミニバッチが使える\n",
    "                #W,B = FC内 FCX.W, FCX.B (Xは1,2,...)\n",
    "                A1 = self.FC1.forward(mini_X_train) #(batch_size, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1) #(batch_size, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1) #(batch_size, n_nodes2)\n",
    "                Z2 = self.activation2.forward(A2) #(batch_size, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2) #(batch_size, n_output)\n",
    "                Z3 = self.activation3.forward(A3) #(batch_size, n_output)                \n",
    "                \n",
    "                dA3 = self.activation3.backward(Z3, mini_y_train) #(batch_size, n_output)\n",
    "                # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3) #(batch_size, n_nodes2)\n",
    "                dA2 = self.activation2.backward(dZ2) #(batch_size, n_nodes2)\n",
    "                dZ1 = self.FC2.backward(dA2) #(batch_size, n_nodes1)\n",
    "                dA1 = self.activation1.backward(dZ1) #(batch_size, n_nodes1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "            \n",
    "            A1 = self.FC1.forward(X) #(sample_size, n_nodes1)\n",
    "            Z1 = self.activation1.forward(A1) #(sample_size, n_nodes1)\n",
    "            A2 = self.FC2.forward(Z1) #(sample_size, n_nodes2)\n",
    "            Z2 = self.activation2.forward(A2) #(sample_size, n_nodes2)\n",
    "            A3 = self.FC3.forward(Z2) #(sample_size, n_output)\n",
    "            Z3 = self.activation3.forward(A3) #(sample_size, n_output)\n",
    "            y_pred_proba = Z3\n",
    "            loss = self.loss_func(y,y_pred_proba)\n",
    "            self.loss = np.append(self.loss,loss)\n",
    "            \n",
    "        self.loss = np.delete(self.loss, 0)\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print(\"self.loss.shape\",self.loss.shape)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.FC1.forward(X) #(n_samples, n_nodes1)\n",
    "        Z1 = self.activation1.forward(A1) #(n_samples, n_nodes1)\n",
    "        A2 = self.FC2.forward(Z1) #(n_samples, n_nodes2)\n",
    "        Z2 = self.activation2.forward(A2) #(n_samples, n_nodes2)\n",
    "        A3 = self.FC3.forward(Z2) #(n_samples, n_output)\n",
    "        Z3 = self.activation3.forward(A3) #(n_samples, n_output)\n",
    "\n",
    "        y_pred_proba = np.argmax(Z3,axis=1)\n",
    "        return y_pred_proba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
