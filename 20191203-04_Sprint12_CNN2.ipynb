{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint12 CNN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from numpy import linalg as LA\n",
    "import copy\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data set\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n",
      "(12000, 28, 28)\n",
      "(48000, 1, 28, 28)\n",
      "(12000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 28, 28)\n",
    "print(X_val.shape) # (12000, 28, 28)\n",
    "\n",
    "'''\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、\n",
    "チャンネル方向の軸は用意しておく必要があります。\n",
    "(n_samples, n_channels, height, width)のNCHW\n",
    "または(n_samples, height, width, n_channels)のNHWC どちらかの形にしてください。\n",
    "'''\n",
    "#(n_samples, n_channels, height, width)のNCHW\n",
    "X_train = X_train.reshape(48000, 1, 28, 28)\n",
    "X_val = X_val.reshape(12000, 1, 28, 28)\n",
    "\n",
    "print(X_train.shape) # (48000, 1, 28, 28)\n",
    "print(X_val.shape) # (12000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】2次元畳み込み層の作成\n",
    ">1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiraki:\n",
    "    def __init__(self, F_size_h, F_size_w, \n",
    "                 stride = 1, padding = 0):\n",
    "        self.F_size_h = F_size_h\n",
    "        self.F_size_w = F_size_w\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.col = None\n",
    "        \n",
    "    def im2col(self, imput): \n",
    "        self.N, self.C, H, W = imput.shape \n",
    "        #(N = n_samples, H = height, W = width, C = channel_1)\n",
    "        self.Nout_h = N_out_h(H, \n",
    "                              self.padding, \n",
    "                              self.F_size_h, \n",
    "                              self.stride) #問題２\n",
    "        self.Nout_w = N_out_w(W, \n",
    "                              self.padding, \n",
    "                              self.F_size_w, \n",
    "                              self.stride) #問題２\n",
    "\n",
    "        #np.pad で　imputデータ　の外堀を埋める。4次元なので4つ引数が必要。\n",
    "        #順番はshape通り。N, C, H, W。\n",
    "        img = np.pad(imput,[(0,0), (0,0), \n",
    "                            (self.padding,self.padding),\n",
    "                            (self.padding,self.padding)])\n",
    "        col = np.zeros((self.N, self.C, \n",
    "                        self.F_size_h, self.F_size_w, \n",
    "                        self.Nout_h, self.Nout_w)) \n",
    "\n",
    "        #strideで移動する分をまとめることでfor回数を減らす。\n",
    "        for y in range(self.F_size_h):\n",
    "            self.y_out = y + self.stride * self.Nout_h\n",
    "            for x in range(self.F_size_w):\n",
    "                self.x_out = x + self.stride * self.Nout_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, \n",
    "                                            y:self.y_out:self.stride, \n",
    "                                            x:self.x_out:self.stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(self.N \n",
    "                                                      * self.Nout_h \n",
    "                                                      * self.Nout_w,\n",
    "                                                      -1)\n",
    "        #reshape(N * Nout_h * Nout_w, -1) = col after (32448000, 9)\n",
    "        self.col = col\n",
    "        return self.col\n",
    "    \n",
    "    def col2im(self, imput):\n",
    "        self.N_2im, self.C_2im, H, W = imput.shape\n",
    "        #入力データの形に戻す\n",
    "        col =  col.reshape(self.N_2im, self.Nout_h, \n",
    "                           self.Nout_w, self.C_2im, \n",
    "                           self.F_size_h, self.F_size_w).transpose(0,3,4,5,1,2)\n",
    "        #入力データを初期化\n",
    "        img = np.zeros((self.N_2im, self.C_2im, \n",
    "                        H + 2 * self.padding + self.stride - 1))\n",
    "        \n",
    "        for y in range(self.F_size_h):\n",
    "            for x in range(self.F_size_w):\n",
    "                img[:,:,y:self.y_out:self.stride,\n",
    "                    x:self.x_out:self.stride] = col[:,:,y,x,:,:]\n",
    "        \n",
    "        return img[:,:,self.padding: H + self.padding, \n",
    "                   self.padding: W + self.padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \"\"\"\n",
    "    チャンネル数を1に限定しない2次元畳み込み層のクラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    channel_1 : int\n",
    "      前の層のchannel数\n",
    "    channel_2 : int\n",
    "      後の層のchannel数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, channel_1, channel_2, padding,\n",
    "                 F_size_h, F_size_w, stride, \n",
    "                 initializer, optimizer):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.channel_1 = channel_1\n",
    "        self.channel_2 = channel_2\n",
    "        self.F_size_w = F_size_w\n",
    "        self.F_size_h = F_size_h\n",
    "        self.stride = stride #stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # initializerのメソッドを使い、self.Filterとself.Bを初期化する\n",
    "        self.Filter = initializer.W(channel_2, \n",
    "                                    channel_1, \n",
    "                                    self.F_size_h, \n",
    "                                    self.F_size_w)\n",
    "        self.B = initializer.B(channel_2) # （出力チャンネル数）\n",
    "        \n",
    "        self.X = None\n",
    "        self.X_array_back = np.zeros_like((self.Filter[0,:,:])) \n",
    "        #backward用\n",
    "        self.X_array = None\n",
    "        im_hiraki = hiraki(self.F_size_h, \n",
    "                           self.F_size_w, \n",
    "                           self.stride, \n",
    "                           self.padding)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, channel_1, H, W)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, channel_2, Nout_h, Nout_w)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        self.H_W = 0\n",
    "        self.H_B = 0\n",
    "        \n",
    "        \n",
    "        self.X_array = im_hiraki.im2col(self.X)\n",
    "        # shape(N * Nout_h * Nout_w, channel_1 * F_size_h * F_size_w)\n",
    "        # after reshape (32448000, 9)\n",
    "        #channelが増えると axis=1 方向に次のチャネルでの行列が加わる。\n",
    "        '''\n",
    "        この時点では im2col でimputした\" X の開き\"を作成しただけ？\n",
    "        重み(Filter)の掛け方は縦行列にして dot 積。\n",
    "        → shape(N * Nout_h * Nout_w, 1) の形で出てくる。\n",
    "        -> .transpose(N, channel, Nout_h, Nout_w)\n",
    "        \n",
    "        → pooling層 = MaxPool2D にぶち込む。\n",
    "        ->poolingでim2col。開きを作る。\n",
    "        →reshape(N, channel_2, Nout_h, Nout_w) の形にして次の畳み込み or 全結合層へ\n",
    "        '''\n",
    "        self.w_filters = self.Filter.flatten() # self.Filterのfaltten\n",
    "        #dot積\n",
    "        A = self.X_array @ self.w_filters + self.B #W(F_size),B(n_nodes2,)\n",
    "        A = A.reshape(im_hiraki.N, \n",
    "                      self.channel_2,\n",
    "                      im_hiraki.Nout_h,\n",
    "                      im_hiraki.Nout_w)\n",
    "        return A #出力 (N, channel_2, Nout_h, Nout_w)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, channel_2, Nout_h, Nout_w)\n",
    "            出力\n",
    "            = 後ろから流れてきた勾配(loss)\n",
    "        Returns\n",
    "        ----------        \n",
    "        dX : 次の形のndarray, shape (X.shape) #4d\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis=1)\n",
    "        \"\"\"\n",
    "        col2im でreshape\n",
    "        dcol(col2im のimput) を4d にreshape\n",
    "        loss @ self.Filter.flatten\n",
    "        を　col2im で画像に戻している。\n",
    "        \n",
    "        ＊転置しないと数値おかしくなる？\n",
    "        各チャンネルの要素和を取る　→　誤差のチャネル\n",
    "        \"\"\"\n",
    "        dcol = dA @ self.X_array\n",
    "        \n",
    "        dA_imgs = im_hiraki.col2im(dcol) #col2im で 6d にreshape\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dW = loss'''loss <- 2d reshaped dA''' @ self.w_filters\n",
    "        \n",
    "        #dWはW.shape(フィルターの形)なので、reshape to 4d が必要。\n",
    "        \n",
    "#         dX = \n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self) \n",
    "        return dX #4d X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 1, 26, 26)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(edgeitems=3)\n",
    "Con_try = Conv2d(channel_1 = 1, channel_2 = 1, padding = 0,\n",
    "                 F_size_h = 3, F_size_w = 3, stride = 1, \n",
    "                 initializer = SimpleInitializer_CNN(),\n",
    "                 optimizer = SGD(lr = 0.01))\n",
    "Con_try.forward(X_train).shape\n",
    "\n",
    "#forward 出力と同じ shape で簡単な数列を loss がわりにする\n",
    "# Con_try.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer_CNN:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_type = 'sig', sigma=0.01):\n",
    "        self.weight_type = weight_type\n",
    "        self.sigma = sigma\n",
    "    def W(self, channel_1, channel_2, \n",
    "          F_size_h, F_size_w):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        #(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "        Parameters\n",
    "        ----------\n",
    "        channel_1 : int\n",
    "          入力チャンネル数\n",
    "        channel_2 : int\n",
    "          出力チャンネル数\n",
    "        F_size : int\n",
    "          フィルターサイズ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        if self.weight_type == 'sig':\n",
    "            W_std = self.sigma\n",
    "        \n",
    "        elif self.weight_type == 'xav':\n",
    "            W_std = 1/np.sqrt(channel_1)\n",
    "            \n",
    "        elif self.weight_type == 'he':\n",
    "            W_std = np.sqrt(2/channel_1)\n",
    "            \n",
    "        W = W_std * np.random.randn(channel_2, channel_1, \n",
    "                                    F_size_h, F_size_w)\n",
    "        return W #(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "    \n",
    "    def B(self, channel_2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        channel_2 : int\n",
    "          出力チャンネル数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : # （出力チャンネル数）\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(channel_2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer): \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        '''\n",
    "        layer (FC)から持ってこれるようになる。\n",
    "        layer.Z = FC内のself.Z\n",
    "        layer.W\n",
    "        layer.B\n",
    "        '''\n",
    "        layer.W = layer.W - self.lr * layer.dW\n",
    "        layer.B = layer.B - self.lr * layer.dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # ミニバッチ方向(axis=0)にベクトルの平均を計算 #(n_nodes1, n_nodes2)\n",
    "        layer.H_W = layer.H_W + np.sum(layer.dW ** 2)\n",
    "        layer.H_B = layer.H_B + np.sum(layer.dB ** 2)\n",
    "        layer.W = layer.W - (self.lr * np.sqrt(1/layer.H_W)) * layer.dW\n",
    "        layer.B = layer.B - (self.lr * np.sqrt(1/layer.H_B)) * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "\n",
    "    def forward(self,A):\n",
    "        if A.ndim == 2:\n",
    "            x = A.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T\n",
    "        \n",
    "#         x = A - np.max(A) \n",
    "        # オーバーフロー対策\n",
    "#         Z_last = np.exp(A) / np.sum(np.exp(A))\n",
    "        return np.exp(A) / np.sum(np.exp(A))\n",
    "        \n",
    "#         return Z_last\n",
    "    \n",
    "    def backward(self,Z_last,y):\n",
    "        dA_last = Z_last - y #交差エントロピー誤差の計算も含む実装\n",
    "        return dA_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "合計が１になるか確認することが大切。\n",
    "\"\"\"\n",
    "soft_func = Softmax()\n",
    "soft = np.sum(soft_func.forward(X_train),axis=1)\n",
    "# soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.A = None    \n",
    "\n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        Z = 1/ (1+ np.exp(- self.A))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        dA =dZ * ((1- (1/ (1+ np.exp(- self.A))))*(1/ (1+ np.exp(- self.A))))\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hipo:\n",
    "    def __init__(self):\n",
    "        self.A = None    \n",
    "\n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        dA = dZ * ((np.tanh(self.A))**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数 ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.forward_A = None\n",
    "    def forward(self, A):\n",
    "        self.forward_A = A\n",
    "        return np.maximum(A, 0)\n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.where(self.forward_A>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】2次元畳み込み後の出力サイズ\n",
    ">畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_out_h(H, padding, F_size_h, stride):\n",
    "    return (H + 2* padding - F_size_h) // stride + 1 #問題２\n",
    "    \n",
    "def N_out_w(W, padding, F_size_w, stride):\n",
    "    return (W + 2* padding - F_size_w) // stride + 1 #問題２"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最大プーリング層の作成\n",
    ">最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    "\n",
    ">ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス \n",
    "(\n",
    "p\n",
    ",\n",
    "q\n",
    ")\n",
    " を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, F_size_h = 2, F_size_w = 2,\n",
    "                 stride = 2, padding = 0):\n",
    "        self.max_index = None \n",
    "        self.F_size_h = F_size_h\n",
    "        self.F_size_w = F_size_w\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, A):\n",
    "        im_hiraki = hiraki(self.F_size_h, \n",
    "                           self.F_size_w, \n",
    "                           self.stride, \n",
    "                           self.padding)\n",
    "        A_array = im_hiraki.im2col(A)\n",
    "        #shape (A_sumples *N_in_h/F_size_h *N_in_w/F_size_w, C*F_size_h*F_size_w)\n",
    "        self.A_max = np.argmax(A_array, axis = 1) #index\n",
    "        #shape (N *N_in_h/F_size_h *N_in_w/F_size_w,)\n",
    "        '''\n",
    "        最大を取ってくる。また、最大値のインデックス (p,q)をforwardからbackに渡す。\n",
    "        A_max_array = A_max.reshape()\n",
    "        '''\n",
    "        return A_max_array  #shape(N, C, N_in_h/F_size_h, N_in_w/F_size_w)\n",
    "    \n",
    "    def backward(self, dX):\n",
    "        '''max のみ返す。他は0で返す'''\n",
    "        #hirakiにする　→　np.zeros_like()[self.A_max]=A_array[self.A_max] ?\n",
    "        \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8112000, 4)\n",
      "(8112000,)\n"
     ]
    }
   ],
   "source": [
    "maxpoo = MaxPool2D()\n",
    "maxpoo.forward(Con_try.forward(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】（アドバンス課題）平均プーリングの作成\n",
    ">平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】平滑化\n",
    ">平滑化するためのFlattenクラスを作成してください。\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        self.shape = Z.shape\n",
    "        #batch size 残す。　\n",
    "        Z.reshape(-1)\n",
    "        return Z　#２d\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        return dA.reshape(self.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定\n",
    ">作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch2dCNNClassifier():\n",
    "    \"\"\"\n",
    "    CNN 分類器\n",
    "\n",
    "    Parameters\n",
    "    ------    \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=5,\n",
    "                 lr=0.01, sigma = 0.01,\n",
    "                 verbose = True):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.iter : 学習回数\n",
    "        \"\"\"\n",
    "        self.iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def loss_func(self,y,y_pred_proba):\n",
    "        sigma_c = np.sum(y * np.log(y_pred_proba), axis = 1) \n",
    "        #batch_size方向に平均\n",
    "        return -np.mean(sigma_c)\n",
    "    \n",
    "    def fit(self, X, y,\n",
    "            sigma = 0.01,\n",
    "            n_output = 10):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_output = n_output\n",
    "        self.channel_1 = 1\n",
    "        self.channel_2 = 1\n",
    "        \n",
    "        optimizer = SGD(self.lr) #更新方法選択\n",
    "        self.Conv1 = Conv2d(channel_1 = 1, channel_2 = 1, padding = 0,\n",
    "                            F_size_h = 3, F_size_w = 3, stride = 1, \n",
    "                            initializer = SimpleInitializer_CNN(),\n",
    "                            optimizer = optimizer)\n",
    "        self.activation1= ReLU()\n",
    "        \n",
    "        self.Conv2 = Conv2d(channel_1 = 1, channel_2 = 1, padding = 0,\n",
    "                            F_size_h = 3, F_size_w = 3, stride = 1, \n",
    "                            initializer = SimpleInitializer_CNN(),\n",
    "                            optimizer = optimizer)\n",
    "        self.activation2= ReLU()\n",
    "        \n",
    "        \n",
    "        self.FC1 = FC(n_nodes1 = self.channel_1,\n",
    "                      n_nodes2 = self.n_output,\n",
    "                      initializer = SimpleInitializer_fullyconnect(weight_type='sig', sigma = self.sigma), \n",
    "                      optimizer = SGD(self.lr))\n",
    "        self.activation3 = Softmax()\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.loss = None\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        for n in range(self.iter): #何回回そう？ MNISTなら50回ほどで試す\n",
    "            get_mini_batch = GetMiniBatch(X_train,\n",
    "                                          y_train_one_hot,\n",
    "                                          batch_size = 20)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "            # このfor文内でミニバッチが使える\n",
    "                #W,B = FC内 FCX.W, FCX.B (Xは1,2,...)\n",
    "                A1 = self.Conv1.forward(mini_X_train) \n",
    "                Z1 = self.activation1.forward(A1) \n",
    "                A2 = self.Conv2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A1) \n",
    "                \"\"\"\n",
    "                ノード　＝　チャンネル数　✖️　チャンネルの要素数\n",
    "                出チャンネル数を　（平滑化）　\n",
    "                \n",
    "                conv\n",
    "                -> pool\n",
    "                -> activate\n",
    "                \n",
    "                -> conv\n",
    "                ...\n",
    "                -> activate\n",
    "                \n",
    "                -> flatten\n",
    "                -> FC\n",
    "                -> activate\n",
    "                \"\"\"\n",
    "                Z2 = self.flatten.forward(Z2)\n",
    "                A3 = self.FC1.forward(Z2) #(batch_size, n_nodes1)\n",
    "                Z3 = self.activation3.forward(A3) #(batch_size, n_nodes1)                \n",
    "                \n",
    "                dA3 = self.activation3.backward(Z2, mini_y_train) #(batch_size, n_output)\n",
    "                # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dX2 = self.FC1.backward(dA3) #(batch_size, n_nodes1)\n",
    "                dA2 = self.activation2.backward(dX2)\n",
    "                dA2 = self.fratten.backward(dA2)\n",
    "                dX1 = self.Conv2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dX1) \n",
    "                dX0 = self.Conv1.backward(dA1) # dZ0は使用しない\n",
    "            \n",
    "            A1 = self.Conv1.forward(X) \n",
    "            Z1 = self.activation1.forward(A1) \n",
    "            A2 = self.Conv2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A1) \n",
    "            Z2 = self.flatten(Z2)\n",
    "            A3 = self.FC1.forward(Z2) #(batch_size, n_nodes1)\n",
    "            Z3 = self.activation3.forward(A3)     \n",
    "            y_pred_proba = Z3\n",
    "            loss = self.loss_func(y,y_pred_proba)\n",
    "            self.loss = np.append(self.loss,loss)\n",
    "            \n",
    "        self.loss = np.delete(self.loss, 0)\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print(\"self.loss.shape\",self.loss.shape)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "        return #y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN2d = Scratch2dCNNClassifier(max_iter=1,\n",
    "                               lr=0.001, sigma = 0.01,\n",
    "                               verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 10)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot.shape #(48000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "CNN2d.fit(X_train, y_train_one_hot)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"processing time:\",t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】（アドバンス課題）LeNet\n",
    ">CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。現在では実用的に使われることはありませんが、歴史的に重要なのは1998年の LeNet です。この構造を再現してMNISTに対して動かし、Accuracyを計算してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】（アドバンス課題）有名な画像認識モデルの調査\n",
    ">CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。\n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。\n",
    "\n",
    "https://keras.io/ja/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】出力サイズとパラメータ数の計算\n",
    ">CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    ">畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。\n",
    "- 7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "- 高さや幅方向を持たない1×1のフィルタの効果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
