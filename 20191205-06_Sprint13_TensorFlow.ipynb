{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint13 TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(7)\n",
    "add = tf.add(a,b)\n",
    "\n",
    "sess = tf.Session()\n",
    "output = sess.run(add)\n",
    "print(output)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Tensor(\"Const:0\", shape=(), dtype=int32)\n",
      "5\n",
      "add Tensor(\"Add:0\", shape=(), dtype=int32)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    outoput = sess.run(add)\n",
    "    print('a',a)\n",
    "    print(a.eval())\n",
    "    print(\"add\",add)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a_n = np.array(5)\n",
    "b_n = np.array(7)\n",
    "output_n = np.add(a_n, b_n)\n",
    "print(output_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlowにおける値の扱い方\n",
    ">TensorFlowが値を扱う上で独自の概念として、placeholderとValiableがあります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholderはデータフローグラフの構築時には値が決まっていないものに使います。最初は配列の形だけ定義しておいて後から値を入れて使う空箱のような存在です。学習ごとに違う値が入る入力データや正解データなどに用いられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "c = tf.placeholder(tf.int32)\n",
    "d = tf.placeholder(tf.int32)\n",
    "add = tf.add(c,d)\n",
    "\n",
    "sess = tf.Session()\n",
    "output = sess.run(add, feed_dict={c:5, d:7})\n",
    "#セッションを実行する際に引数feed_dictを使い、placeholderに入れる値を辞書型で指定します。\n",
    "#ここを書き換えることで異なる計算が可能になります。\n",
    "print(output)\n",
    "\n",
    "output = sess.run(add, feed_dict={c:20, d:32})\n",
    "print(output)\n",
    "#ミニバッチ学習を行うような場合を想定すると、　placeholder の必要性がわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">《Valiable》\n",
    "Valiableはplaceholderとは違い、データフローグラフの構築時にも値を持ち、更新を行うものに対して使います。学習するパラメータ（重み、バイアス）に用いられます。\n",
    "\n",
    ">《constant》\n",
    "確認になりますが、placeholderでもValiableでもないただの値は定数constantとして扱います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "add = a + b\n",
    "output = sess.run(add)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### セッションのインスタンス化から終了までに対して、with構文を使うことも可能です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.Session() as sess:\\n    sess.run() # ここに計算の実行コードを入れていく\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with tf.Session() as sess:\n",
    "    sess.run() # ここに計算の実行コードを入れていく\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 論理回路\n",
    ">簡単な題材として、ロジスティック回帰による論理回路の再現を行います。論理回路は2つの値を入力し、1つの値を出力する関数のようなものです。入力も出力も0か1のみで表され、入力される組み合わせによって出力する値が変わります。\n",
    "ANDゲートは入力された2つの値が両方とも1だった場合、出力が1となり、それ以外の組み合わせでは0を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]]) #(4, 2)\n",
    "y_train = np.array([[0],[0],[0],[1]]) #(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データフローグラフの構築\n",
    ">学習データをTensorFlowのデータフローグラフに入力するための placeholder を用意しましょう。placeholderはデータフローグラフを作成する段階では値が決まっていない、空箱のような存在でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "第一引数のtf.float32で行列要素の数値のデータ型を指定しています。\n",
    "第二引数の[None,2]で行列の形を指定しています。\n",
    "ここで定義されている2はデータの次元を表しています。Noneの部分はデータ数を表す部分です。\n",
    "今回のANDゲートの場合のデータ数は[0,0],[0,1],[1,0],[1,1]の4つしかないので\n",
    "Noneの部分を[4,2]としても問題はありません。\n",
    "しかし、任意の数のデータを入れられるように、一般的にはNoneを使います。\n",
    "'''\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">重みとバイアスの Valiable を用意します。Valiableとして用意するということは、これらが学習により更新を行う値であることを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">次にモデルの出力y（＝仮定関数）と目的関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.sigmoid(tf.matmul(x,W) + b)\n",
    "#tf.matmul()はNumPyにおけるnp.dot()に相当する\n",
    "#ベクトルの内積や、行列積を計算するためのメソッドです。\n",
    "\n",
    "cross_entropy = tf.reduce_sum(-t * tf.log(y) \n",
    "                              - (1 - t) * tf.log(1 - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ここまでで、入力のための空箱であるplaceholderと学習可能なValiableをメソッドで結ぶことができました。\n",
    "学習を行うために、勾配降下法を用いてパラメータを最適化するためのコードを加えます。目的関数をGradientDescentOptimizerに渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "#GradientDescentOptimizer()は引数で学習率を指定しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">学習後の結果の正解が正しいかどうかの判定と正解率の計算もデータフローグラフとして定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1行目で結果が正解かどうか判定しています。\\n一つ一つ見ていきましょう。\\nまずtf.equal()は引数に指定された2つの値が等しいかどうかを判定してくれます。\\n返り値はBool値です。tf.sign()は引数の値が正なら1、0なら0、負なら-1を返します。\\nyが0.5以上かどうかで結果が決まるので、y-0.5とt-0.5の符号を比較しています。\\n\\n2行目は正解率を計算するためのコードです。\\ntf.reduce_mean()は多次元配列の各成分の平均を計算する関数です。\\ntf.cast()でBool値を0,1に変換しています。\\nつまりここでは正解で1、不正解で0と判定された配列の平均値をとっているので\\n正解率を表していることになります。\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\"\"\"\n",
    "1行目で結果が正解かどうか判定しています。\n",
    "一つ一つ見ていきましょう。\n",
    "まずtf.equal()は引数に指定された2つの値が等しいかどうかを判定してくれます。\n",
    "返り値はBool値です。tf.sign()は引数の値が正なら1、0なら0、負なら-1を返します。\n",
    "yが0.5以上かどうかで結果が決まるので、y-0.5とt-0.5の符号を比較しています。\n",
    "\n",
    "2行目は正解率を計算するためのコードです。\n",
    "tf.reduce_mean()は多次元配列の各成分の平均を計算する関数です。\n",
    "tf.cast()でBool値を0,1に変換しています。\n",
    "つまりここでは正解で1、不正解で0と判定された配列の平均値をとっているので\n",
    "正解率を表していることになります。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを入力して計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nまずセッションのインスタンスを作成します。\\nそして、tf.global_variables_initializer()によって\\n上で定義したtf.Variable()の値(重み・バイアス)を初期化します。\\n実行する際にはsess.run()を使います。\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "'''\n",
    "まずセッションのインスタンスを作成します。\n",
    "そして、tf.global_variables_initializer()によって\n",
    "上で定義したtf.Variable()の値(重み・バイアス)を初期化します。\n",
    "実行する際にはsess.run()を使います。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Accuracy: 0.750000\n",
      "epoch: 100, Accuracy: 1.000000\n",
      "epoch: 200, Accuracy: 1.000000\n",
      "epoch: 300, Accuracy: 1.000000\n",
      "epoch: 400, Accuracy: 1.000000\n",
      "epoch: 500, Accuracy: 1.000000\n",
      "epoch: 600, Accuracy: 1.000000\n",
      "epoch: 700, Accuracy: 1.000000\n",
      "epoch: 800, Accuracy: 1.000000\n",
      "epoch: 900, Accuracy: 1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n2行目、sess.run()に上で定義したtrain_stepを入れることで、\\n勾配降下法による学習を行っています。\\n8行目はsess.run()にaccuracyを入れることで、正解率を計算しています。\\n計算結果がNumPy形式で返ってきているので、これをprintします。\\n\\n形だけ定義していたplaceholderのxとtの中には値が何も入っていません。\\nplaceholderに値を設定するためにsess.run()のパラメータでfeed_dictを指定します。\\n例えば、feed_dict={x:x_train,t:y_train})と書くことで\\n空箱だったxにx_trainの値が入り、tにy_trainの値が入ります。\\n\\n表示結果を見てみると、正解率が100%でうまく学習できているように見えます。\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学習を行います。今回は1000回繰り返すことにします。\n",
    "\n",
    "for epoch in range(1000):\n",
    "    sess.run(train_step, feed_dict = {\n",
    "        x:x_train,\n",
    "        t:y_train})\n",
    "    \n",
    "    # 100回ごとに正解率を表示\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(\n",
    "            accuracy, feed_dict = {\n",
    "                x:x_train,\n",
    "                t:y_train})\n",
    "        print('epoch: %d, Accuracy: %f'\n",
    "              %(epoch, acc_val))\n",
    "        \n",
    "'''\n",
    "2行目、sess.run()に上で定義したtrain_stepを入れることで、\n",
    "勾配降下法による学習を行っています。\n",
    "8行目はsess.run()にaccuracyを入れることで、正解率を計算しています。\n",
    "計算結果がNumPy形式で返ってきているので、これをprintします。\n",
    "\n",
    "形だけ定義していたplaceholderのxとtの中には値が何も入っていません。\n",
    "placeholderに値を設定するためにsess.run()のパラメータでfeed_dictを指定します。\n",
    "例えば、feed_dict={x:x_train,t:y_train})と書くことで\n",
    "空箱だったxにx_trainの値が入り、tにy_trainの値が入ります。\n",
    "\n",
    "表示結果を見てみると、正解率が100%でうまく学習できているように見えます。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">各サンプルの計算結果を確認してロジスティック回帰の実装を終わります。先ほどのaccuracyと同様、実行することで計算結果が返ってくるためprintします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[1.9654632e-04]\n",
      " [4.9049824e-02]\n",
      " [4.9049824e-02]\n",
      " [9.3120384e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nclassifiedの結果は全てTrueで正しく学習されていることがわかります。\\nprobは上からほぼ[0,0,0,1]となっています。\\n今回活性化関数に用いたのはシグモイド関数ですので出力yは確率として表示されています。\\n上から3つは1になる確率がほぼ0％、一番下の1つは93％程度の確率で1になるということになります。\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学習結果が正しいか確認\n",
    "classified = sess.run(correct_prediction, feed_dict = {\n",
    "    x:x_train,\n",
    "    t:y_train})\n",
    "\n",
    "#出力yの確認\n",
    "prob = sess.run(y, feed_dict = {\n",
    "    x:x_train,\n",
    "    t:y_train})\n",
    "\n",
    "print(classified)\n",
    "print(prob)\n",
    "'''\n",
    "classifiedの結果は全てTrueで正しく学習されていることがわかります。\n",
    "probは上からほぼ[0,0,0,1]となっています。\n",
    "今回活性化関数に用いたのはシグモイド関数ですので出力yは確率として表示されています。\n",
    "上から3つは1になる確率がほぼ0％、一番下の1つは93％程度の確率で1になるということになります。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[5.5699544]\n",
      " [5.5699544]]\n",
      "b: [-8.534579]\n"
     ]
    }
   ],
   "source": [
    "print('W:',sess.run(W))\n",
    "print('b:',sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 途中の値が見たい場合\n",
    ">デバッグのために途中の値が見たい場合もあります。例えば、y = tf.sigmoid(tf.matmul(x, W) + b)のtf.matmul(x, W)の部分が見たいといったことを考えます。\n",
    "出力yがsess.run(y, feed_dict={x:x_train, t:y_train})で見れたことと同様の行えば良いため、次のようなコードになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.       ]\n",
      " [ 5.5699544]\n",
      " [ 5.5699544]\n",
      " [11.139909 ]]\n"
     ]
    }
   ],
   "source": [
    "mat = tf.matmul(x, W)\n",
    "y = tf.sigmoid(mat + b)\n",
    "print(sess.run(mat, feed_dict = {\n",
    "    x:x_train,\n",
    "    t:y_train}))\n",
    "sess.close()\n",
    "\n",
    "#データフローグラフを構築し、それをsess.run()で実行するという流れはここでも同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint13 DLFlameWork 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.array([[0,0],[0,1],[1,0],[1,1]]) #(4, 2)\n",
    "# y_train = np.array([[0],[0],[0],[1]]) #(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】スクラッチを振り返る\n",
    ">ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重みの初期化が必要だった\n",
    "#層ごとに使う関数とハイパーパラメーターを設定しなければならなかった\n",
    "#ミニバッジとエポックでループする必要があった。\n",
    "#クラス作成やクラス内関数を数多く作らなければならなかった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの用意\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from numpy import linalg as LA\n",
    "import copy\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "#print(iris)\n",
    "\n",
    "X = pd.DataFrame(iris.data,columns=iris.feature_names) #(150, 4)\n",
    "\n",
    "y = pd.DataFrame(iris.target,columns={'Species'}) #(150, 1)\n",
    "#print(iris.target_names) #['setosa' 'versicolor' 'virginica']\n",
    "df = pd.concat([X, y], axis=1) #(150, 5)\n",
    "data = df[df['Species'] !=0] #(100, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】スクラッチとTensorFlowの対応を考える\n",
    ">先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 0, loss : 56.5591, val_loss : 26.5573, acc : 0.250, val_acc : 0.625\n",
      "Epoch 1, loss : 12.0300, val_loss : 25.6138, acc : 0.750, val_acc : 0.375\n",
      "Epoch 2, loss : 19.1267, val_loss : 10.8235, acc : 0.250, val_acc : 0.625\n",
      "Epoch 3, loss : 6.6376, val_loss : 13.9818, acc : 0.750, val_acc : 0.375\n",
      "Epoch 4, loss : 7.9972, val_loss : 5.9252, acc : 0.500, val_acc : 0.688\n",
      "Epoch 5, loss : 6.1204, val_loss : 12.6583, acc : 0.750, val_acc : 0.375\n",
      "Epoch 6, loss : 3.1816, val_loss : 3.5615, acc : 0.500, val_acc : 0.750\n",
      "Epoch 7, loss : 2.1083, val_loss : 4.5454, acc : 0.750, val_acc : 0.562\n",
      "Epoch 8, loss : 0.4695, val_loss : 2.3228, acc : 0.750, val_acc : 0.812\n",
      "Epoch 9, loss : 0.1924, val_loss : 2.4479, acc : 0.750, val_acc : 0.750\n",
      "test_acc : 0.700\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 損失関数の出力をトレース対象とする\n",
    "# loss_summary = tf.summary.scalar('loss', loss)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #TENSOR BOARD\n",
    "    tf.summary.scalar('cross_entropy', loss_op)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter('data', graph = sess.graph)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "#             val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "        \n",
    "        summary_str = sess.run(summary_op, feed_dict={X: X_val, Y: y_val})\n",
    "        summary_writer.add_summary(summary_str, epoch)\n",
    "        \n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "    \n",
    "    summary_writer.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1206 16:53:36.378894 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.400886 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.425184 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.444333 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.474127 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.494716 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.515990 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.534233 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.561155 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.577843 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.607084 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.621618 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.652225 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.669901 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "TensorBoard 1.15.0 at http://AkinoMacBook-puro.local:8088/ (Press CTRL+C to quit)\n",
      "W1206 16:53:36.694233 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.710523 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.736713 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.756190 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.783900 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.801088 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.826400 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.845396 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.870381 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.883373 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.899456 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.902070 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.914322 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.916481 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.925466 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.926891 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.934658 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.935768 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.943608 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.944864 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.951900 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.952466 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.963684 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.963906 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 16:53:36.973922 123145398910976 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 16:53:36.974327 123145398910976 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir=data --port=8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, total_loss : 2.9373, total_val_loss : 2.1309, total_acc : 0.407, total_val_acc : 0.446\n",
      "Epoch 1, total_loss : 0.5168, total_val_loss : 0.4098, total_acc : 0.757, total_val_acc : 0.804\n",
      "Epoch 2, total_loss : 0.4277, total_val_loss : 0.4271, total_acc : 0.729, total_val_acc : 0.732\n",
      "Epoch 3, total_loss : 0.4903, total_val_loss : 0.3992, total_acc : 0.671, total_val_acc : 0.777\n",
      "Epoch 4, total_loss : 0.3156, total_val_loss : 0.2443, total_acc : 0.857, total_val_acc : 0.982\n",
      "Epoch 5, total_loss : 0.2110, total_val_loss : 0.1981, total_acc : 0.957, total_val_acc : 0.982\n",
      "Epoch 6, total_loss : 0.2676, total_val_loss : 0.2115, total_acc : 0.900, total_val_acc : 0.929\n",
      "Epoch 7, total_loss : 0.1916, total_val_loss : 0.1539, total_acc : 0.943, total_val_acc : 0.991\n",
      "Epoch 8, total_loss : 0.1741, total_val_loss : 0.1451, total_acc : 0.957, total_val_acc : 0.991\n",
      "Epoch 9, total_loss : 0.1870, total_val_loss : 0.1431, total_acc : 0.914, total_val_acc : 0.991\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    # 1エポック内でのイテレーション回数を返す\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    # 各イテレーションで使うバッチデータを返す\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    # イテレーションのカウント数をゼロにする\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    # 次のイテレーションで使うバッチデータを返す\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(dtype=\"float\", shape=[None, n_input])\n",
    "Y = tf.placeholder(dtype=\"float\", shape=[None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # he_normal()の初期化\n",
    "    initializer = tf.initializers.he_uniform()\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.get_variable(name='W1', shape=[n_input, n_hidden1], initializer=initializer),\n",
    "        'w2': tf.get_variable(name='W2', shape=[n_hidden1, n_hidden2], initializer=initializer),\n",
    "        'w3': tf.get_variable(name='W3', shape=[n_hidden2, 1], initializer=initializer)\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.get_variable(name='b1', shape=[n_hidden1], initializer=initializer),\n",
    "        'b2': tf.get_variable(name='b2', shape=[n_hidden2], initializer=initializer),\n",
    "        'b3': tf.get_variable(name='b3', shape=[1], initializer=initializer)\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # summaryの設定\n",
    "    \n",
    "#     # 指定したディレクトリがあれば削除し、再作成\n",
    "#     if tf.gfile.Exists(log_dir):\n",
    "#         tf.gfile.DeleteRecursively(log_dir)\n",
    "# 　　　　tf.gfile.MakeDirs(log_dir)\n",
    "    \n",
    "    tf.summary.scalar('cross_entropy', loss_op)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter('data', graph=sess.graph)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        total_val_loss /= total_batch\n",
    "        total_val_acc /= total_batch    \n",
    "        #tf.summary.FileWriter('iris_sigmoid', sess.graph)\n",
    "        summary_str = sess.run(summary_op, feed_dict={X: X_val, Y: y_val})\n",
    "        summary_writer.add_summary(summary_str, epoch)\n",
    "        print(\"Epoch {}, total_loss : {:.4f}, total_val_loss : {:.4f}, total_acc : {:.3f}, total_val_acc : {:.3f}\".format(\n",
    "            epoch, total_loss, total_val_loss, total_acc, total_val_acc))\n",
    "    # tensorboardでスカラを表示させるために必要\n",
    "    summary_writer.flush()  \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1206 17:19:05.507390 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.542107 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.570373 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.591423 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.618563 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.640835 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.665759 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.683169 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.708719 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.725196 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.752377 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.765841 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.791248 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.808449 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "TensorBoard 1.15.0 at http://AkinoMacBook-puro.local:6006/ (Press CTRL+C to quit)\n",
      "W1206 17:19:05.835203 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.850693 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.878896 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.898286 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.928090 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:05.946795 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:05.975970 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.002254 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.032694 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.048702 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.066869 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.069563 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.082182 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.084748 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.094985 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.096623 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.104448 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.105581 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.114210 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.116003 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.124547 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.125025 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.133086 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.133304 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.142298 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.142574 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.150076 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.150322 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.159351 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.159651 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1206 17:19:06.169155 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.169493 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.180128 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.180448 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.190329 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.190554 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W1206 17:19:06.198277 123145340358656 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W1206 17:19:06.198487 123145340358656 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backwardの作成が不要。手間が半分。\n",
    "#重みとバイアスの初期化をまとめて行っているため、見やすい。\n",
    "#それぞれの段階で何を行っているのかが見やすい。\n",
    "#計算が関数で用意されているので関数作成の手間が大幅に削減されている。\n",
    "#ループ、ハイパーパラメーターは相変わらず手作業での実装が必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    ">risデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下の2箇所は2クラス分類特有の処理です。\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akishimasaki/.pyenv/versions/anaconda3-2019.03/envs/py37/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, loss:17.2233,        val_loss:6.1041, total_acc:0.506, val_acc:0.542\n",
      "Epoch1, loss:2.1804,        val_loss:1.7385, total_acc:0.521, val_acc:0.667\n",
      "Epoch2, loss:0.0020,        val_loss:0.6543, total_acc:0.667, val_acc:0.667\n",
      "Epoch3, loss:0.0000,        val_loss:1.7449, total_acc:0.673, val_acc:0.667\n",
      "Epoch4, loss:0.0003,        val_loss:2.3319, total_acc:0.683, val_acc:0.708\n",
      "Epoch5, loss:0.0063,        val_loss:2.9026, total_acc:0.690, val_acc:0.736\n",
      "Epoch6, loss:2.8167,        val_loss:5.8170, total_acc:0.693, val_acc:0.750\n",
      "Epoch7, loss:0.0000,        val_loss:0.9056, total_acc:0.684, val_acc:0.694\n",
      "Epoch8, loss:0.0000,        val_loss:2.8221, total_acc:0.678, val_acc:0.708\n",
      "Epoch9, loss:0.0000,        val_loss:2.9801, total_acc:0.681, val_acc:0.708\n",
      "test_acc:0.711\n"
     ]
    }
   ],
   "source": [
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \n",
    "               \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = df[\"Species\"]\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.array(X) #(150, 4)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis] \n",
    "\n",
    "\"\"\"\n",
    "one_hot化が必要\n",
    "\"\"\"\n",
    "onehot = OneHotEncoder(sparse=False)\n",
    "y = onehot.fit_transform(y) #one_hot(150, 3)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, \n",
    "                                    batch_size = batch_size)\n",
    "\n",
    "def net_3_layers(x):\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1':tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2':tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3':tf.Variable(tf.random_normal([n_hidden2, n_classes]))}\n",
    "    biases = {'b1':tf.Variable(tf.random_normal([n_hidden1])),\n",
    "              'b2':tf.Variable(tf.random_normal([n_hidden2])),\n",
    "              'b3':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']),biases['b1']) \n",
    "    #tf.matmul = @, dot積\n",
    "    layer_1 = tf.nn.relu(layer_1) \n",
    "    #tf.nn.relu -> フィルタやバイアスを足しこんだ結果(Wx + b)のテンソルを利用する\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2) #softmaxに変更\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = net_3_layers(X)\n",
    "\n",
    "'''\n",
    "loss_op, correct_pred を多値分類用に変更\n",
    "softmax\n",
    "'''\n",
    "\n",
    "# 目的関数\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "                         (labels = Y, logits = logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), \n",
    "                        tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "#tf.equal 渡された２つのベクトルが一致しているか否かを見る。返り値[True, False]。\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化 *大切\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #np.ceil 切り上げ\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, \n",
    "                mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            #enumerate()関数を使うと、\n",
    "            #forループの中でリスト（配列）などのイテラブルオブジェクトの要素と同時に\n",
    "            #インデックス番号（カウント、順番）を取得できる。\n",
    "            sess.run(train_op, \n",
    "                     feed_dict = {X: mini_batch_x, \n",
    "                                  Y: mini_batch_y})\n",
    "            #tf.placeholder()値は、sess.run()のfeed_dictに辞書型で指定\n",
    "            loss, acc = sess.run([loss_op, accuracy],\n",
    "                                 feed_dict = {X: mini_batch_x, \n",
    "                                              Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        \n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy],\n",
    "                                     feed_dict = {X: X_val,\n",
    "                                                  Y: y_val})\n",
    "        print(\"Epoch{}, loss:{:.4f},\\\n",
    "        val_loss:{:.4f}, total_acc:{:.3f}, val_acc:{:.3f}\"\n",
    "              .format(epoch, loss, val_loss, total_acc, val_acc))\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict = {X: X_test,\n",
    "                                               Y: y_test})\n",
    "    print(\"test_acc:{:.3f}\".format(test_acc))\n",
    "    \n",
    "    \n",
    "#tensorboad で結果見てみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】House Pricesのモデルを作成\n",
    ">回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
    "分類問題と回帰問題の違いを考慮してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Epoch0, loss:1204820224.0000,        val_loss:2498041600.0000\n",
      "Epoch1, loss:1328785408.0000,        val_loss:2532409088.0000\n",
      "Epoch2, loss:1269164800.0000,        val_loss:2514784768.0000\n",
      "Epoch3, loss:1210956032.0000,        val_loss:2503635456.0000\n",
      "Epoch4, loss:1212569472.0000,        val_loss:2493856256.0000\n",
      "Epoch5, loss:1214144896.0000,        val_loss:2494605056.0000\n",
      "Epoch6, loss:1229514624.0000,        val_loss:2499302912.0000\n",
      "Epoch7, loss:1223481216.0000,        val_loss:2494694144.0000\n",
      "Epoch8, loss:1234858496.0000,        val_loss:2495266304.0000\n",
      "Epoch9, loss:1194202496.0000,        val_loss:2494798848.0000\n",
      "test_loss:3967448064.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "初期化の順番が大事な時はinitialized_valueを使うこと。\n",
    "\n",
    "\"\"\"\n",
    "# データセットの読み込み\n",
    "data = pd.read_csv('train.csv')\n",
    "y = data['SalePrice'] #(1460,)\n",
    "y = y.astype(np.int)[:, np.newaxis] #(1460, 1)\n",
    "X = data.loc[:,['GrLivArea','YearBuilt']] #(1460, 2)\n",
    "y = np.array(y).astype(np.float64)\n",
    "X = np.array(X).astype(np.float64)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "#分類問題と回帰問題の違いを考慮してください。\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(y_test.dtype)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "\n",
    "'''\n",
    "n_classes = 3　を回帰問題にする時にどうする？ -> クラスではなく 1 で固定。\n",
    "'''\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, n_input])\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 1])\n",
    "\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, \n",
    "                                    batch_size = batch_size)\n",
    "\n",
    "def net_houseplice(x):\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1':tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2':tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3':tf.Variable(tf.random_normal([n_hidden2, 1]))}\n",
    "    biases = {'b1':tf.Variable(tf.random_normal([n_hidden1])),\n",
    "              'b2':tf.Variable(tf.random_normal([n_hidden2])),\n",
    "              'b3':tf.Variable(tf.random_normal([1]))}\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']),biases['b1']) \n",
    "    #tf.matmul = @, dot積\n",
    "    layer_1 = tf.nn.relu(layer_1) \n",
    "    #tf.nn.relu -> フィルタやバイアスを足しこんだ結果(Wx + b)のテンソルを利用する\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = net_houseplice(X) \n",
    "\n",
    "\n",
    "'''\n",
    "loss_op, correct_pred を回帰問題用に変更\n",
    "平均二乗誤差（MSE: Mean Squared Error）は回帰問題に使われる一般的な損失関数\n",
    "回帰問題の一般的な評価指標は平均絶対誤差（MAE: Mean Absolute Error）\n",
    "\n",
    "一般にラプラス分布は正規分布よりも裾野が広く、多くの外れ値が観測されます。\n",
    "多くの外れ値が存在するデータの誤差を評価したい、\n",
    "あるいは外れ値にあまり影響されない評価を行いたい場合、\n",
    "RMSE より MAE のほうが優れた指標であるといえるでしょう。\n",
    "\n",
    "他方、基本的な線形回帰(MLR, PLS)から deep learning まで、\n",
    "最小化する関数には二乗誤差が使われていることも見逃せません。\n",
    "'''\n",
    "\n",
    "# 目的関数 MSE\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 指標値計算\n",
    "#精度、評価　RMSE\n",
    "# RMSE = tf.sqrt(loss_op)\n",
    "#tf.sqrt -> √\n",
    "\n",
    "# variableの初期化 *大切\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #np.ceil 切り上げ\n",
    "        total_loss = 0\n",
    "        for i, (mini_batch_x, \n",
    "                mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            #enumerate()関数を使うと、\n",
    "            #forループの中でリスト（配列）などのイテラブルオブジェクトの要素と同時に\n",
    "            #インデックス番号（カウント、順番）を取得できる。\n",
    "            sess.run(train_op,\n",
    "                     feed_dict = {X: mini_batch_x, \n",
    "                                  Y: mini_batch_y})\n",
    "            #tf.placeholder()値は、sess.run()のfeed_dictに辞書型で指定\n",
    "            loss = sess.run(loss_op,\n",
    "                            feed_dict = {X: mini_batch_x, \n",
    "                                         Y: mini_batch_y})\n",
    "            \n",
    "            total_loss += loss\n",
    "            \n",
    "        \n",
    "        total_loss /= total_batch\n",
    "        val_loss = sess.run(loss_op,\n",
    "                            feed_dict = {X: X_val,\n",
    "                                         Y: y_val})\n",
    "        print(\"Epoch{}, loss:{:.4f},\\\n",
    "        val_loss:{:.4f}\".format(epoch, loss, val_loss))\n",
    "    \n",
    "    test_loss = sess.run(loss_op, feed_dict = {X: X_test,\n",
    "                                               Y: y_test})\n",
    "    print(\"test_loss:{:.3f}\".format(test_loss))\n",
    "    \n",
    "#対数取ってない、正規化してないのでloss でかい\n",
    "#tensorboad で結果見てみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTのモデルを作成\n",
    ">ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "スクラッチで実装したモデルの再現を目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n",
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "#入力が画像である\n",
    "# MNIST data set\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64\n",
    "X_train = X_train.reshape(-1, 784) /255\n",
    "X_test = X_test.reshape(-1, 784) /255\n",
    "\n",
    "'''\n",
    "ニューラルネットワークに入れる数字が大きいとうまく働かないことがあるので、\n",
    "正規化した方が良い\n",
    "'''\n",
    "\n",
    "\n",
    "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, loss:0.5086,        val_loss:0.9717, total_acc:0.712, val_acc:0.713\n",
      "Epoch1, loss:0.5939,        val_loss:0.8216, total_acc:0.768, val_acc:0.796\n",
      "Epoch2, loss:0.1736,        val_loss:0.5039, total_acc:0.850, val_acc:0.888\n",
      "Epoch3, loss:0.0229,        val_loss:0.3571, total_acc:0.902, val_acc:0.913\n",
      "Epoch4, loss:0.0084,        val_loss:0.3203, total_acc:0.919, val_acc:0.920\n",
      "Epoch5, loss:0.0458,        val_loss:0.3891, total_acc:0.928, val_acc:0.913\n",
      "Epoch6, loss:0.0135,        val_loss:0.3078, total_acc:0.932, val_acc:0.928\n",
      "Epoch7, loss:0.0175,        val_loss:0.3005, total_acc:0.935, val_acc:0.929\n",
      "Epoch8, loss:0.0048,        val_loss:0.3091, total_acc:0.938, val_acc:0.926\n",
      "Epoch9, loss:0.0275,        val_loss:0.3227, total_acc:0.939, val_acc:0.928\n",
      "test_acc:0.931\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, n_input])\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, n_classes])\n",
    "Y_ = tf.placeholder(dtype = tf.float32, shape = [None, n_classes])\n",
    "\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, \n",
    "                                    batch_size = batch_size)\n",
    "\n",
    "def net_3_layers(x):\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1':tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2':tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3':tf.Variable(tf.random_normal([n_hidden2, n_classes]))}\n",
    "    biases = {'b1':tf.Variable(tf.random_normal([n_hidden1])),\n",
    "              'b2':tf.Variable(tf.random_normal([n_hidden2])),\n",
    "              'b3':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']),biases['b1']) \n",
    "    #tf.matmul = @, dot積\n",
    "    layer_1 = tf.nn.relu(layer_1) \n",
    "    #tf.nn.relu -> フィルタやバイアスを足しこんだ結果(Wx + b)のテンソルを利用する\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2) #softmaxに変更\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = net_3_layers(X)\n",
    "\n",
    "'''\n",
    "loss_op, correct_pred を多値分類用に変更\n",
    "softmax\n",
    "'''\n",
    "\n",
    "# 目的関数\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "                         (labels = Y, logits = logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1),tf.argmax(Y, 1))\n",
    "#tf.equal 渡された２つのベクトルが一致しているか否かを見る。返り値[True, False]。\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化 *大切\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #np.ceil 切り上げ\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, \n",
    "                mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            #enumerate()関数を使うと、\n",
    "            #forループの中でリスト（配列）などのイテラブルオブジェクトの要素と同時に\n",
    "            #インデックス番号（カウント、順番）を取得できる。\n",
    "            sess.run(train_op, \n",
    "                     feed_dict = {X: mini_batch_x, \n",
    "                                  Y: mini_batch_y})\n",
    "            #tf.placeholder()値は、sess.run()のfeed_dictに辞書型で指定\n",
    "            loss, acc = sess.run([loss_op, accuracy],\n",
    "                                 feed_dict = {X: mini_batch_x, \n",
    "                                              Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        \n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy],\n",
    "                                     feed_dict = {X: X_val,\n",
    "                                                  Y: y_val_one_hot})\n",
    "        print(\"Epoch{}, loss:{:.4f},\\\n",
    "        val_loss:{:.4f}, total_acc:{:.3f}, val_acc:{:.3f}\"\n",
    "              .format(epoch, loss, val_loss, total_acc, val_acc))\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict = {X: X_test,\n",
    "                                               Y: y_test_one_hot})\n",
    "    print(\"test_acc:{:.3f}\".format(test_acc))\n",
    "    \n",
    "    \n",
    "#tensorboad で結果見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN ver\n",
    "# https://qiita.com/yoyoyo_/items/7ecd9ef446d255c7d866 に回答あり\n",
    "\n",
    "'''\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、\n",
    "チャンネル方向の軸は用意しておく必要があります。\n",
    "(n_samples, n_channels, height, width)のNCHW\n",
    "または(n_samples, height, width, n_channels)のNHWC どちらかの形にしてください。\n",
    "'''\n",
    "#(n_samples, n_channels, height, width)のNCHW\n",
    "X_train = X_train.reshape(48000, 1, 28, 28)\n",
    "X_val = X_val.reshape(12000, 1, 28, 28)\n",
    "\n",
    "print(X_train.shape) # (48000, 1, 28, 28)\n",
    "print(X_val.shape) # (12000, 1, 28, 28)\n",
    "\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, \n",
    "                                    batch_size = batch_size)\n",
    "\n",
    "\n",
    "def conv2d# CNN ver\n",
    "# https://qiita.com/yoyoyo_/items/7ecd9ef446d255c7d866 に回答あり(x, F_si\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "def conv_net(x, F_size = 3, \n",
    "           channel_in = 1, channel_out = 1,\n",
    "           strides = 1):\n",
    "    weights = {\n",
    "        'w1':tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2':tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3':tf.Variable(tf.random_normal([n_hidden2, n_classes]))}\n",
    "    biases = {'b1':tf.Variable(tf.random_normal([n_hidden1])),\n",
    "              'b2':tf.Variable(tf.random_normal([n_hidden2])),\n",
    "              'b3':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']),biases['b1']) \n",
    "    #tf.matmul = @, dot積\n",
    "    layer_1 = tf.nn.relu(layer_1) \n",
    "    #tf.nn.relu -> フィルタやバイアスを足しこんだ結果(Wx + b)のテンソルを利用する\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2) #softmaxに変更\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = conv_net(X)\n",
    "\n",
    "'''\n",
    "loss_op, correct_pred を多値分類用に変更\n",
    "softmax\n",
    "'''\n",
    "\n",
    "# 目的関数\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "                         (labels = Y, logits = logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), \n",
    "                        tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "#tf.equal 渡された２つのベクトルが一致しているか否かを見る。返り値[True, False]。\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化 *大切\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        #np.ceil 切り上げ\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, \n",
    "                mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            #enumerate()関数を使うと、\n",
    "            #forループの中でリスト（配列）などのイテラブルオブジェクトの要素と同時に\n",
    "            #インデックス番号（カウント、順番）を取得できる。\n",
    "            sess.run(train_op, \n",
    "                     feed_dict = {X: mini_batch_x, \n",
    "                                  Y: mini_batch_y})\n",
    "            #tf.placeholder()値は、sess.run()のfeed_dictに辞書型で指定\n",
    "            loss, acc = sess.run([loss_op, accuracy],\n",
    "                                 feed_dict = {X: mini_batch_x, \n",
    "                                              Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        \n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy],\n",
    "                                     feed_dict = {X: X_val,\n",
    "                                                  Y: y_val})\n",
    "        print(\"Epoch{}, loss:{:.4f},\\\n",
    "        val_loss:{:.4f}, total_acc:{:.3f}, val_acc:{:.3f}\"\n",
    "              .format(epoch, loss, val_loss, total_acc, val_acc))\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict = {X: X_test,\n",
    "                                               Y: y_test})\n",
    "    print(\"test_acc:{:.3f}\".format(test_acc))\n",
    "    \n",
    "    \n",
    "#tensorboad で結果見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
