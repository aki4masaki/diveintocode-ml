{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint15 論文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Region proposal methods typically rely on inexpensive features and economical inference schemes.\n",
    "Selective Search [4], one of the most popular methods, greedily merges superpixels based on engineered\n",
    "low-level features. Yet when compared to efficient\n",
    "detection networks [2], Selective Search is an order of\n",
    "magnitude slower, at 2 seconds per image in a CPU\n",
    "implementation. EdgeBoxes [6] currently provides the\n",
    "best tradeoff between proposal quality and speed,\n",
    "at 0.2 seconds per image. Nevertheless, the region\n",
    "proposal step still consumes as much running time\n",
    "as the detection network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Object Proposals. There is a large literature on object\n",
    "proposal methods. Comprehensive surveys and comparisons of object proposal methods can be found in\n",
    "[19], [20], [21]. Widely used object proposal methods\n",
    "include those based on grouping super-pixels (e.g.,\n",
    "Selective Search [4], CPMC [22], MCG [23]) and those\n",
    "based on sliding windows (e.g., objectness in windows\n",
    "[24], EdgeBoxes [6]). Object proposal methods were\n",
    "adopted as external modules independent of the detectors (e.g., Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [2])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Abstract This paper addresses the problem of generating\n",
    "possible object locations for use in object recognition. We\n",
    "introduce selective search which combines the strength of\n",
    "both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling\n",
    "process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate\n",
    "possible object locations, we diversify our search and use a\n",
    "variety of complementary image partitionings to deal with\n",
    "as many image conditions as possible. Our selective search\n",
    "results in a small set of data-driven, class-independent, high\n",
    "quality locations, yielding 99% recall and a Mean Average\n",
    "Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables\n",
    "the use of stronger machine learning techniques and stronger\n",
    "appearance models for object recognition. In this paper we\n",
    "show that our selective search enables the use of the powerful\n",
    "Bag-of-Words model for recognition. The selective search\n",
    "software is made publicly available (Software: http://disi.\n",
    "unitn.it/~uijlings/SelectiveSearch.html).\\\n",
    "(J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders, “Selective search for object recognition,” International Journal of Computer Vision (IJCV), 2013.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "- Selective Search(J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders, “Selective search for object recognition,” International Journal of Computer Vision (IJCV), 2013.)\n",
    "- CPMC\n",
    "- MCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
    "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
    "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
    "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
    "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
    "“attention” mechanisms, the RPN component tells the unified network where to look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "- 従来の Selective Search の代わりに Region Proposal Network (RPN)を導入するという仕組み。CNNを行う回数を大幅に減らすことで高速化を実現した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The OverFeat paper [9] proposes a detection\n",
    "method that uses regressors and classifiers on sliding\n",
    "windows over convolutional feature maps. OverFeat\n",
    "is a one-stage, class-specific detection pipeline, and ours\n",
    "is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To compare the one-stage and two-stage systems,\n",
    "we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by\n",
    "one-stage Fast R-CNN. In this system, the “proposals”\n",
    "are dense sliding windows of 3 scales (128, 256, 512)\n",
    "and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is\n",
    "trained to predict class-specific scores and regress box\n",
    "locations from these sliding windows. Because the\n",
    "OverFeat system adopts an image pyramid, we also\n",
    "evaluate using convolutional features extracted from\n",
    "5 scales. We use those 5 scales as in [1], [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">two stage型はR-FCNの流れを汲む手法で、対象オブジェクトの領域候補を作るstageとCNNを用いた認識を行うstageのふたつを持つものです。もうひとつのone stage型はYoloやSSDの流れを汲み、すべての学習をone stageでやってしまうものです。\\\n",
    "(https://www.sigfoss.com/developer_blog/detail?actual_object_id=247)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "- one-stage はクラスに特化した検出経路のみで学習し物体検出を行う一方、 two-stage はクラスに依存しない提案とクラスに特化した検出からなる経路(流れ)の二つで学習し物体検出を行う。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">3.1 Region Proposal Networks\n",
    "A Region Proposal Network (RPN) takes an image\n",
    "(of any size) as input and outputs a set of rectangular\n",
    "object proposals, each with an objectness score.3 We\n",
    "model this process with a fully convolutional network\n",
    "[7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN\n",
    "object detection network [2], we assume that both nets\n",
    "share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model\n",
    "[32] (ZF), which has 5 shareable convolutional layers\n",
    "and the Simonyan and Zisserman model [3] (VGG-16),\n",
    "which has 13 shareable convolutional layers.\n",
    "To generate region proposals, we slide a small\n",
    "network over the convolutional feature map output\n",
    "by the last shared convolutional layer. This small\n",
    "network takes as input an n × n spatial window of\n",
    "the input convolutional feature map. Each sliding\n",
    "window is mapped to a lower-dimensional feature\n",
    "(256-d for ZF and 512-d for VGG, with ReLU [33]\n",
    "following). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a\n",
    "box-classification layer (cls). We use n = 3 in this\n",
    "paper, noting that the effective receptive field on the\n",
    "input image is large (171 and 228 pixels for ZF and\n",
    "VGG, respectively). This mini-network is illustrated\n",
    "at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window\n",
    "fashion, the fully-connected layers are shared across\n",
    "all spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed\n",
    "by two sibling 1 × 1 convolutional layers (for reg and\n",
    "cls, respectively).\n",
    "\n",
    ">Figure 3: Left: Region Proposal Network (RPN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "- Region Proposal Network (RPN)。ROI生成のための手法で、従来の Selective Search とは異なり特徴マップの生成前に一度だけCNNを実行するもの。end-to-end 学習の実現にも貢献している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) RoIプーリングとは何か。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nevertheless, our method achieves bounding-box\n",
    "regression by a different manner from previous RoIbased (Region of Interest) methods [1], [2]. In [1],\n",
    "[2], bounding-box regression is performed on features\n",
    "pooled from arbitrarily sized RoIs, and the regression\n",
    "weights are shared by all region sizes. In our formulation, the features used for regression are of the same\n",
    "spatial size (3 × 3) on the feature maps. To account\n",
    "for varying sizes, a set of k bounding-box regressors\n",
    "are learned. Each regressor is responsible for one scale\n",
    "and one aspect ratio, and the k regressors do not share\n",
    "weights. As such, it is still possible to predict boxes of\n",
    "various sizes even though the features are of a fixed\n",
    "size/scale, thanks to the design of anchors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">RoI Poolingでは、ある程度畳み込み処理を行ったfeature mapから、region proposalにあたる部分領域をうまく「固定サイズのfeature map」として抽出します。\\\n",
    "(Qiita https://qiita.com/yu4u/items/5cbe9db166a5d72f9eb8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The RoI pooling layer uses max pooling to convert the\n",
    "features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7),\n",
    "where H and W are layer hyper-parameters that are independent of any particular RoI. In this paper, an RoI is a\n",
    "rectangular window into a conv feature map. Each RoI is\n",
    "defined by a four-tuple (r, c, h, w) that specifies its top-left\n",
    "corner (r, c) and its height and width (h, w).\n",
    "\n",
    ">RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate\n",
    "size h/H × w/W and then max-pooling the values in each\n",
    "sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel,\n",
    "as in standard max pooling. The RoI layer is simply the\n",
    "special-case of the spatial pyramid pooling layer used in\n",
    "SPPnets [11] in which there is only one pyramid level. We\n",
    "use the pooling sub-window calculation given in [11]\\\n",
    "(R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "- ＊(R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.)\n",
    "- ある程度畳み込み処理を行ったfeature mapから、region proposalにあたる部分領域を「固定サイズのfeature map」として抽出すること。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Anchorのサイズはどうするのが適切か。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">we use 3 scales and\n",
    "3 aspect ratios, yielding k = 9 anchors at each sliding\n",
    "position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "- 複数スケール、アスペクト比のアンカーを用意することが適切。\n",
    "- デフォルトでは３x３"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">4.3 From MS COCO to PASCAL VOC\n",
    "Large-scale data is of crucial importance for improving deep neural networks. Next, we investigate how\n",
    "the MS COCO dataset can help with the detection\n",
    "performance on PASCAL VOC.\n",
    "As a simple baseline, we directly evaluate the\n",
    "COCO detection model on the PASCAL VOC dataset,\n",
    "without fine-tuning on any PASCAL VOC data. This\n",
    "evaluation is possible because the categories on\n",
    "COCO are a superset of those on PASCAL VOC. The\n",
    "categories that are exclusive on COCO are ignored in\n",
    "this experiment, and the softmax layer is performed\n",
    "only on the 20 categories plus background. The mAP\n",
    "under this setting is 76.1% on the PASCAL VOC 2007\n",
    "test set (Table 12). This result is better than that trained\n",
    "on VOC07+12 (73.2%) by a good margin, even though\n",
    "the PASCAL VOC data are not exploited.\n",
    "Then we fine-tune the COCO detection model on\n",
    "the VOC dataset. In this experiment, the COCO model\n",
    "is in place of the ImageNet-pre-trained model (that\n",
    "is used to initialize the network weights), and the\n",
    "Faster R-CNN system is fine-tuned as described in\n",
    "Section 3.2. Doing so leads to 78.8% mAP on the\n",
    "PASCAL VOC 2007 test set. The extra data from\n",
    "the COCO set increases the mAP by 5.6%. Table 6\n",
    "shows that the model trained on COCO+VOC has\n",
    "the best AP for every individual category on PASCAL\n",
    "VOC 2007. Similar improvements are observed on the\n",
    "PASCAL VOC 2012 test set (Table 12 and Table 7). We\n",
    "note that the test-time speed of obtaining these strong\n",
    "results is still about 200ms per image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "\n",
    "training data 2007 test 2012 test\\\n",
    "VOC07              69.9      67.0\\\n",
    "VOC07+12           73.2         -\\\n",
    "VOC07++12             -      70.4\\\n",
    "COCO (no VOC)      76.1      73.0\\\n",
    "COCO+VOC07+12      78.8         -\\\n",
    "COCO+VOC07++12        -      75.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The first one is featurizing image pyramids (i.e. a series of resized copies of the input image) to produce semantically representative multi-scale features. Features from\n",
    "images of different scales yield predictions separately and\n",
    "these predictions work together to give the final prediction.In terms of recognition accuracy and localization precision,\n",
    "features from various-sized images do surpass features that\n",
    "are based merely on single-scale images. Methods such as\n",
    "(Shrivastava et al. 2016) and SNIP (Singh and Davis 2018)\n",
    "employed this tactic. Despite the performance gain, such a\n",
    "strategy could be costly time-wise and memory-wise, which\n",
    "forbid its application in real-time tasks. Considering this\n",
    "major drawback, methods such as SNIP (Singh and Davis2018) can choose to only employ featurized image pyramids\n",
    "during the test phase as a fallback, whereas other methods\n",
    "including Fast R-CNN (Girshick 2015) and Faster R-CNN\n",
    "(Ren et al. 2015) chose not to use this strategy by default.\\\n",
    "(M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network\n",
    "Qijie Zhao, Tao Sheng, Yongtao Wang, Zhi Tang, Ying Chen, Ling Cai, Haibin Ling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer\n",
    "- M2det -> featurizing image pyramids の説明時に、Faster R-CNN では使用されていなかったという一文で引用されている。また、M2det の IoU,AP の比較対象として引用されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
