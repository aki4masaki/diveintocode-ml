{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint22 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】SimpleRNNのフォワードプロパゲーション実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "a\n",
    "t\n",
    " : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "h\n",
    "t\n",
    " : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "x\n",
    "t\n",
    " : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "W\n",
    "x\n",
    " : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "h\n",
    "t\n",
    "−\n",
    "1\n",
    " : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "W\n",
    "h\n",
    " : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "B\n",
    " : バイアス項 (n_nodes,)\n",
    "\n",
    "初期状態 \n",
    "h\n",
    "0\n",
    " は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 \n",
    "x\n",
    " は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from numpy import linalg as LA\n",
    "import copy\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練用データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 1, \n",
    "                 seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_type = 'xav', sigma=0.01):\n",
    "        self.weight_type = weight_type\n",
    "        self.sigma = sigma\n",
    "    def W_x(self, n_features, n_nodes):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "          X の N (n_features)\n",
    "        n_nodes : int\n",
    "          出力ノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W_x : W_x # (n_features, n_nodes)\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        \n",
    "        if self.weight_type == 'sig':\n",
    "            self.W_std = self.sigma\n",
    "        \n",
    "        elif self.weight_type == 'xav':\n",
    "            self.W_std = 1/np.sqrt(n_features)\n",
    "            \n",
    "        elif self.weight_type == 'he':\n",
    "            self.W_std = np.sqrt(2/n_features)\n",
    "            \n",
    "        W_x = self.W_std * np.random.randn(self.n_features, self.n_nodes)\n",
    "        return W_x\n",
    "    \n",
    "    def W_h(self):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "          X の N (n_features)\n",
    "        n_nodes : int\n",
    "          出力ノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W_h : W_h # (n_nodes, n_nodes)\n",
    "        \"\"\"\n",
    "        W_h = self.W_std * np.random.randn(self.n_nodes, self.n_nodes)\n",
    "        return W_h # (n_nodes, n_nodes)\n",
    "    \n",
    "    def B(self):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(self.n_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "\n",
    "    def forward(self,A):\n",
    "        if A.ndim == 2:\n",
    "            x = A.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T\n",
    "        \n",
    "#         x = A - np.max(A) \n",
    "        # オーバーフロー対策\n",
    "#         Z_last = np.exp(A) / np.sum(np.exp(A))\n",
    "        return np.exp(A) / np.sum(np.exp(A))\n",
    "        \n",
    "#         return Z_last\n",
    "    \n",
    "    def backward(self,Z_last,y):\n",
    "        dA_last = Z_last - y #交差エントロピー誤差の計算も含む実装\n",
    "        return dA_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49750002, 0.50249998]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "合計が１になるか確認することが大切。\n",
    "\"\"\"\n",
    "soft_func = Softmax()\n",
    "soft = np.sum(soft_func.forward(x),axis=1)\n",
    "soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hipo:\n",
    "    def __init__(self):\n",
    "        self.A = None    \n",
    "\n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        dA = dZ * ((np.tanh(self.A))**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes, \n",
    "                 initializer = False, optimizer = False):\n",
    "        self.optimizer = optimizer\n",
    "        if initializer:\n",
    "            # initializerのメソッドを使い、self.W_x, self.W_h, self.Bを初期化する\n",
    "            self.W_x = initializer.W_x(n_features, n_nodes)\n",
    "            self.W_h = initializer.W_h()\n",
    "            self.B = initializer.B()\n",
    "        else:\n",
    "            self.W_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "            self.W_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "            self.B = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "\n",
    "        self.X = None\n",
    "        \n",
    "    def forward(self, X, h_s):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        h_s : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        self.h_s = h_s\n",
    "        A = self.X @ self.W_x + self.h_s @ self.W_h + self.B \n",
    "        # W_x(n_features, n_nodes), W_h(n_nodes, n_nodes), B(n_nodes,)\n",
    "        \n",
    "        return A #出力 (batch_size, n_nodes)\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis=0)\n",
    "        self.dW = self.X.T @ dA\n",
    "        dZ = dA @ self.W.T\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self) \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer): \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        '''\n",
    "        layer (FC)から持ってこれるようになる。\n",
    "        layer.Z = FC内のself.Z\n",
    "        layer.W\n",
    "        layer.B\n",
    "        '''\n",
    "        layer.W_x = layer.W_x - self.lr * layer.dW_x\n",
    "        layer.W_h = layer.W_h - self.lr * layer.dW_h\n",
    "        layer.B = layer.B - self.lr * layer.dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "スクラッチ\n",
    "'''    \n",
    "\n",
    "class ScratchSimpleRNNClassifier:\n",
    "    def __init__(self, \n",
    "                 W_x = 0,\n",
    "                 W_h = 0,\n",
    "                 max_iter = 1,\n",
    "                 lr = 0.01, sigma = 0.01,\n",
    "                 verbose = False):\n",
    "        self.iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "    \n",
    "#     def loss_func(self,y,y_pred_proba):\n",
    "#         sigma_c = np.sum(y * np.log(y_pred_proba), axis = 1) \n",
    "#         #batch_size方向に平均\n",
    "#         pass\n",
    "    \n",
    "#         return #-np.mean(sigma_c)\n",
    "    \n",
    "    def fit(self, X, y= False, sigma = 0.01,\n",
    "            n_nodes = 4,\n",
    "            X_val = False, y_val = False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_sequence, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            訓練用データの正解値\n",
    "        \"\"\"\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_features = X.shape[2]\n",
    "        self.n_sequence = X.shape[1]\n",
    "        self.batch_size = X.shape[0]\n",
    "        \n",
    "        self.h_s = np.zeros((self.batch_size, \n",
    "                             self.n_sequence,\n",
    "                             self.n_nodes)) # shape(1,3,4)\n",
    "        \n",
    "#         optimizer = SGD(self.lr) #更新方法選択\n",
    "        self.FC = FC(self.n_features,\n",
    "                     self.n_nodes,\n",
    "                     optimizer = optimizer)\n",
    "        self.activation = Hipo()\n",
    "    \n",
    "        for i in range(self.n_sequence):\n",
    "            for n in range(self.iter): #何回回そう？ MNISTなら50回ほどで試す\n",
    "                A = self.FC.forward(X[:,i,:],self.h_s[:,i-1,:]) #(batch_size, n_nodes)\n",
    "                h_s = self.activation.forward(A) #(batch_size, n_nodes)\n",
    "                self.h_s[:,i,:] = h_s\n",
    "                \n",
    "                #問題3 back \n",
    "                \n",
    "            print(\"{}回目時点の出力:{}\".format(i, h_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "\n",
    "# print(\"x\",x)\n",
    "# print(x.shape)\n",
    "# print(\"w_x\",w_x)\n",
    "# print(\"w_h\",w_h)\n",
    "# print(\"h.shape\",h.shape)\n",
    "# print(\"b.shape\",b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = ScratchSimpleRNNClassifier(max_iter=1,\n",
    "                                 lr=0.01,\n",
    "                                 verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_i [[0.01 0.02]]\n",
      "X_i (1, 2)\n",
      "0回目時点の出力:[[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
      "X_i [[0.02 0.03]]\n",
      "X_i (1, 2)\n",
      "1回目時点の出力:[[0.792209   0.8141834  0.83404912 0.84977719]]\n",
      "X_i [[0.03 0.04]]\n",
      "X_i (1, 2)\n",
      "2回目時点の出力:[[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "processing time: 0.002191781997680664\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "RNN.fit(x)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"processing time:\",t2-t1)\n",
    "\n",
    "# y_pred = NN.predict(X_test)\n",
    "# print(\"accuracy\",accuracy_score(np.argmax(y_test_one_hot,axis=1),y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#仮のLOSSを各時間ごとに自作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    def __init__(self, max_iter=5,\n",
    "                 lr=0.01, sigma = 0.01,\n",
    "                 verbose = True):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.iter : 学習回数\n",
    "        \"\"\"\n",
    "        self.iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def loss_func(self,y,y_pred_proba):\n",
    "        sigma_c = np.sum(y * np.log(y_pred_proba), axis = 1) \n",
    "        #batch_size方向に平均\n",
    "        return -np.mean(sigma_c)\n",
    "    \n",
    "    def fit(self, X, y,\n",
    "            sigma = 0.01,n_nodes1 = 400,n_nodes2 = 200,\n",
    "            n_output = 10, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        optimizer = SGD(self.lr) #更新方法選択\n",
    "        \n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1,\n",
    "                      SimpleInitializer(weight_type='xav',sigma = self.sigma),optimizer)\n",
    "        self.activation1 = Hipo()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, \n",
    "                      SimpleInitializer(weight_type='xav',sigma = self.sigma),optimizer)\n",
    "        self.activation2 = Hipo()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, \n",
    "                      SimpleInitializer(weight_type='xav',sigma = self.sigma),optimizer)\n",
    "        self.activation3 = Hipo()\n",
    "        self.loss = None\n",
    "        \n",
    "        for n in range(self.iter): #何回回そう？ MNISTなら50回ほどで試す\n",
    "            get_mini_batch = GetMiniBatch(X_train,\n",
    "                                          y_train_one_hot,\n",
    "                                          batch_size=20)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "            # このfor文内でミニバッチが使える\n",
    "                #W,B = FC内 FCX.W, FCX.B (Xは1,2,...)\n",
    "                A1 = self.FC1.forward(mini_X_train) #(batch_size, n_nodes1)\n",
    "                Z1 = self.activation1.forward(A1) #(batch_size, n_nodes1)\n",
    "                A2 = self.FC2.forward(Z1) #(batch_size, n_nodes2)\n",
    "                Z2 = self.activation2.forward(A2) #(batch_size, n_nodes2)\n",
    "                A3 = self.FC3.forward(Z2) #(batch_size, n_output)\n",
    "                Z3 = self.activation3.forward(A3) #(batch_size, n_output)                \n",
    "                \n",
    "                dA3 = self.activation3.backward(Z3, mini_y_train) #(batch_size, n_output)\n",
    "                # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3) #(batch_size, n_nodes2)\n",
    "                dA2 = self.activation2.backward(dZ2) #(batch_size, n_nodes2)\n",
    "                dZ1 = self.FC2.backward(dA2) #(batch_size, n_nodes1)\n",
    "                dA1 = self.activation1.backward(dZ1) #(batch_size, n_nodes1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "            \n",
    "            A1 = self.FC1.forward(X) #(sample_size, n_nodes1)\n",
    "            Z1 = self.activation1.forward(A1) #(sample_size, n_nodes1)\n",
    "            A2 = self.FC2.forward(Z1) #(sample_size, n_nodes2)\n",
    "            Z2 = self.activation2.forward(A2) #(sample_size, n_nodes2)\n",
    "            A3 = self.FC3.forward(Z2) #(sample_size, n_output)\n",
    "            Z3 = self.activation3.forward(A3) #(sample_size, n_output)\n",
    "            y_pred_proba = Z3\n",
    "            loss = self.loss_func(y,y_pred_proba)\n",
    "            self.loss = np.append(self.loss,loss)\n",
    "            \n",
    "        self.loss = np.delete(self.loss, 0)\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print(\"self.loss.shape\",self.loss.shape)\n",
    "       \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.FC1.forward(X) #(n_samples, n_nodes1)\n",
    "        Z1 = self.activation1.forward(A1) #(n_samples, n_nodes1)\n",
    "        A2 = self.FC2.forward(Z1) #(n_samples, n_nodes2)\n",
    "        Z2 = self.activation2.forward(A2) #(n_samples, n_nodes2)\n",
    "        A3 = self.FC3.forward(Z2) #(n_samples, n_output)\n",
    "        Z3 = self.activation3.forward(A3) #(n_samples, n_output)\n",
    "\n",
    "        y_pred_proba = np.argmax(Z3,axis=1)\n",
    "        return y_pred_proba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
